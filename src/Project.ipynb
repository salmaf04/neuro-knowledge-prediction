{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9u-8wpmphD-",
    "outputId": "74f11504-5a86-4b10-e7c5-73c0024af182"
   },
   "outputs": [],
   "source": [
    "# %% [Instalaci√≥n completa]\n",
    "!apt-get update\n",
    "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-spa\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install pykeen pdf2image pytesseract Pillow nltk networkx matplotlib transformers torch huggingface-hub  pymupdf unidecode spacy owlready2\n",
    "!pip install langdetect\n",
    "# Descarga recursos NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')  # √ötil para an√°lisis adicional\n",
    "\n",
    "!python -m spacy download es_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIbhgW9laFc9",
    "outputId": "7a1ced23-30f8-4f01-fa95-3d09d5104520",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS0KdqPPLD2x"
   },
   "source": [
    "# Procesamiento del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOhFI_fMgTka"
   },
   "outputs": [],
   "source": [
    "STOPWORDS = set(\n",
    "    [\n",
    "        \"introducci√≥n\",\n",
    "        \"m√©todo\",\n",
    "        \"m√©todos\",\n",
    "        \"resultado\",\n",
    "        \"resultados\",\n",
    "        \"discusi√≥n\",\n",
    "        \"conclusi√≥n\",\n",
    "        \"figura\",\n",
    "        \"tabla\",\n",
    "        \"referencia\",\n",
    "        \"estudio\",\n",
    "        \"an√°lisis\",\n",
    "        \"datos\",\n",
    "        \"art√≠culo\",\n",
    "        \"secci√≥n\",\n",
    "        \"mostrado\",\n",
    "        \"usando\",\n",
    "        \"usado\",\n",
    "        \"basado\",\n",
    "        \"encontrado\",\n",
    "        \"tambi√©n\",\n",
    "        \"sin embargo\",\n",
    "        \"aunque\",\n",
    "        \"a√±o\",\n",
    "        \"a√±os\",\n",
    "        \"tiempo\",\n",
    "        \"alto\",\n",
    "        \"bajo\",\n",
    "        \"valor\",\n",
    "        \"caso\",\n",
    "        \"grupo\",\n",
    "        \"et\",\n",
    "        \"al\",\n",
    "        \"probabilidad\",\n",
    "        \"momento\",\n",
    "        \"situaciones\",\n",
    "        \"descubrir\",\n",
    "        \"mantiene\",\n",
    "        \"significaba\",\n",
    "        \"quiz√°s\",\n",
    "        \"debido\",\n",
    "        \"uso\",\n",
    "        \"hacer\",\n",
    "        \"obtener\",\n",
    "        \"puede\",\n",
    "        \"podr√≠a\",\n",
    "        \"listado\",\n",
    "        \"conferencias\",\n",
    "        \"antecedentes\",\n",
    "        \"significancia\",\n",
    "        \"derechos de autor\",\n",
    "        \"autor\",\n",
    "        \"fig\",\n",
    "        \"ec\",\n",
    "        \"vol\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "BLACKLIST = {\n",
    "    \"fig\", \"figura\", \"figure\", \"tabla\", \"table\", \"cuadro\", \"doi\", \"issn\",\n",
    "    \"url\", \"http\", \"www\", \"et\", \"al\", \"vol\", \"no\", \"p√°g\", \"pag\", \"ed\",\n",
    "    \"estudio\", \"an√°lisis\", \"datos\", \"m√©todo\", \"resultado\", \"conclusi√≥n\" # Palabras gen√©ricas\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2wZpWLZjvkwR",
    "outputId": "1cb0b3c4-2fa9-4c99-ba88-6df1797cedcf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SIHUwScfiDw",
    "outputId": "eea46d5f-8f23-4352-f732-57245651de5b"
   },
   "outputs": [],
   "source": [
    "from idna import decode\n",
    "import pdf2image\n",
    "import fitz\n",
    "import pytesseract\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "import hashlib\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import string\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_lg\")\n",
    "    print(\"‚úÖ Modelo spaCy 'es_core_news_lg' cargado correctamente.\")\n",
    "except OSError:\n",
    "    print(\"‚ö†Ô∏è Error cargando spaCy. Aseg√∫rate de haber ejecutado la celda de instalaci√≥n.\")\n",
    "\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "ruta_local = os.path.expanduser(\"./model\")\n",
    "# Configurar la ruta de la carpeta\n",
    "carpeta_txts = \"/content/drive/MyDrive/txts\"  # Cambiado a Google Drive\n",
    "carpeta_pdfs = Path(\"/content/drive/MyDrive/corpus\")\n",
    "carpeta_pdfs = Path(carpeta_pdfs)\n",
    "article_txt = \"\"\n",
    "all_articles_text = []\n",
    "\n",
    "# Asegurarse de que la carpeta existe\n",
    "if not carpeta_pdfs.exists():\n",
    "    carpeta_pdfs.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Carpeta '{carpeta_pdfs}' creada. Coloca tus PDFs all√≠.\")\n",
    "else:\n",
    "    # Procesar todos los PDFs en la carpeta\n",
    "    # Initialize article_txt to ensure it's set for each iteration or if no PDFs are processed\n",
    "    article_txt = \"\"  # Esta variable se usar√° globalmente, pero la reiniciaremos por PDF\n",
    "\n",
    "    for pdf_file in carpeta_pdfs.glob(\"*.pdf\"):\n",
    "        print(f\"\\nProcesando: {pdf_file.name}\")\n",
    "\n",
    "        article_txt = \"\"  # Variable local para cada PDF\n",
    "\n",
    "        try:\n",
    "            # Construir nombre del archivo txt\n",
    "            pdf_file_name = pdf_file.with_suffix(\".txt\").name\n",
    "            file_name = Path(carpeta_txts) / pdf_file_name # FIX: Changed './txts' to 'carpeta_txts'\n",
    "\n",
    "            print(file_name)\n",
    "\n",
    "            # Si ya existe el .txt, cargarlo\n",
    "            if file_name.exists():\n",
    "                print(f\"  El archivo {file_name.name} ya existe. Cargando texto existente...\")\n",
    "                with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                    article_txt = f.read()\n",
    "            else:\n",
    "                # --- Extracci√≥n nueva del PDF ---\n",
    "                doc = fitz.open(pdf_file)\n",
    "                full_text_pages = []\n",
    "\n",
    "                for page in doc:\n",
    "                    blocks = page.get_text(\"blocks\", sort=True)\n",
    "                    page_text_parts = []\n",
    "                    for block in blocks:\n",
    "                        txt = block[4]\n",
    "                        if not txt.strip() or len(txt) < 5:\n",
    "                            continue\n",
    "                        txt = re.sub(r'([a-z√°√©√≠√≥√∫√±])([A-Z√Å√â√ç√ì√ö√ë])', r'\\1 \\2', txt)\n",
    "                        page_text_parts.append(txt)\n",
    "\n",
    "                    full_text_pages.append(\"\\n\".join(page_text_parts))\n",
    "\n",
    "                article_txt = \"\\n\".join(full_text_pages)\n",
    "                article_txt = re.sub(r'\\s+', ' ', article_txt).strip()\n",
    "\n",
    "                # Guardar el texto extra√≠do\n",
    "                file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(article_txt)\n",
    "\n",
    "                print(f\"  Texto extra√≠do y guardado en {file_name.name}\")\n",
    "\n",
    "            # Agregar a la lista de todos los textos\n",
    "            if article_txt.strip():\n",
    "                print(article_txt)  # Solo si no est√° vac√≠o\n",
    "                all_articles_text.append(article_txt)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error procesando {pdf_file.name}: {e}\")\n",
    "\n",
    "# Combinar todos los textos\n",
    "combined_text = \"\\n\\n\".join(all_articles_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "svIc-0qxfyo8",
    "outputId": "589c705d-6ece-4671-84a9-51ccd6a58e28"
   },
   "outputs": [],
   "source": [
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "def is_spanish(text):\n",
    "    try:\n",
    "        # Si el texto es muy corto, langdetect falla o es impreciso\n",
    "        if len(text.split()) < 4:\n",
    "            return False\n",
    "        return detect(text) == 'es'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Mejora en la limpieza de texto para unir palabras partidas por guiones\n",
    "def clean_text(text):\n",
    "    text = re.split(r'\\n\\s*(Referencia|Bibliograf|Bibliography|References)', text, flags=re.IGNORECASE)[0]\n",
    "    text = re.sub(r'(\\w)-\\n(\\w)', r'\\1\\2', text)\n",
    "\n",
    "    lines = text.split('\\n')\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        # Filtra l√≠neas que sean solo n√∫meros o muy cortas (basura de paginaci√≥n)\n",
    "        if len(line.strip()) < 3 or line.strip().isdigit():\n",
    "            continue\n",
    "        clean_lines.append(line.strip())\n",
    "\n",
    "    text = \" \".join(clean_lines)\n",
    "\n",
    "    # 4. Limpieza est√°ndar\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(combined_text)\n",
    "ctext = clean_text(combined_text)\n",
    "sentences = nltk.tokenize.sent_tokenize(ctext, language=\"spanish\")\n",
    "print(sentences)\n",
    "print(ctext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrMAio3LLD2z"
   },
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NU8F8cO5yRec"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Reemplazo de spaCy: Cargar pipeline de Hugging Face para NER biom√©dico en espa√±ol\n",
    "model_name = \"HUMADEX/spanish_medical_ner\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "    # 2. Pasar el tokenizador expl√≠cito al pipeline\n",
    "    ner_pipeline = pipeline(\n",
    "        \"ner\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        aggregation_strategy=\"max\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error cargando modelo: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "61vzRB_DyWU5",
    "outputId": "ce971d10-5389-4862-84f4-59ebac834829"
   },
   "outputs": [],
   "source": [
    "entity_list = []\n",
    "\n",
    "ENGLISH_STOPWORDS = {\"the\", \"of\", \"and\", \"in\", \"to\", \"with\", \"a\", \"for\", \"study\", \"fistulae\", \"skull\", \"base\", \"flap\", \"nasoseptal\"}\n",
    "STOPWORDS.update(ENGLISH_STOPWORDS)\n",
    "\n",
    "entity_list = []\n",
    "\n",
    "# Tokenizar oraciones\n",
    "sentences = nltk.tokenize.sent_tokenize(ctext, language=\"spanish\")\n",
    "\n",
    "for s in sentences:\n",
    "    # 1. FILTRO DE IDIOMA: Si la oraci√≥n no es espa√±ol, s√°ltala completamente\n",
    "    if not is_spanish(s):\n",
    "        continue\n",
    "\n",
    "    if len(s) < 20: continue\n",
    "\n",
    "    # Ejecutar NER\n",
    "    results = ner_pipeline(s)\n",
    "    doc = nlp(s)\n",
    "\n",
    "    denotations = []\n",
    "    seen_lemmas = set() # Para evitar duplicados en la misma oraci√≥n, pero sin romper orden interno\n",
    "\n",
    "    for ent in results:\n",
    "        if ent[\"score\"] < 0.60: continue\n",
    "\n",
    "        # Obtener span de spaCy para lematizaci√≥n precisa\n",
    "        span = doc.char_span(ent[\"start\"], ent[\"end\"], alignment_mode=\"contract\")\n",
    "\n",
    "        if not span:\n",
    "            # Fallback simple si spaCy no alinea: usar la palabra cruda\n",
    "            lemma = ent[\"word\"].strip().lower()\n",
    "        else:\n",
    "            # --- CORRECCI√ìN CR√çTICA AQU√ç ---\n",
    "            # NO usar set(). Usar lista para mantener orden: \"mucosa nasal\" != \"nasal mucosa\"\n",
    "            lemma_parts = [t.lemma_.lower() for t in span]\n",
    "            lemma = \" \".join(lemma_parts)\n",
    "            lemma = unidecode(lemma) # Quitar acentos para normalizar grafo\n",
    "\n",
    "        # Limpieza final del lema\n",
    "        lemma = re.sub(r'[^\\w\\s]', '', lemma).strip() # Quitar puntuaci√≥n\n",
    "\n",
    "        # Filtros de calidad del nodo\n",
    "        if (len(lemma) < 4 or\n",
    "            lemma in STOPWORDS or\n",
    "            lemma in BLACKLIST or\n",
    "            lemma.isdigit() or\n",
    "            any(w in ENGLISH_STOPWORDS for w in lemma.split())): # Si contiene palabras en ingl√©s\n",
    "            continue\n",
    "\n",
    "        label = ent[\"entity_group\"]\n",
    "\n",
    "        # Evitar agregar el mismo nodo dos veces para la misma oraci√≥n\n",
    "        if lemma not in seen_lemmas:\n",
    "            denotations.append({\n",
    "                \"obj\": label,\n",
    "                \"span\": {\"begin\": ent[\"start\"], \"end\": ent[\"end\"]},\n",
    "                \"lemma\": lemma, # Ahora es \"cirugia endoscopica\", no \"endoscopica cirugia\"\n",
    "                \"original\": ent[\"word\"]\n",
    "            })\n",
    "            seen_lemmas.add(lemma)\n",
    "\n",
    "    if denotations:\n",
    "        entity_list.append({\"text\": s, \"denotations\": denotations})\n",
    "\n",
    "print(entity_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HusPN8eZluk6"
   },
   "outputs": [],
   "source": [
    "parsed_entities = []\n",
    "for entities_in_sentence in entity_list:\n",
    "    e = []\n",
    "    # If there are not entities in the text\n",
    "    if not entities_in_sentence.get(\"denotations\"):\n",
    "        parsed_entities.append(\n",
    "            {\n",
    "                \"text\": entities_in_sentence[\"text\"],\n",
    "                \"text_sha256\": hashlib.sha256(\n",
    "                    entities_in_sentence[\"text\"].encode(\"utf-8\")\n",
    "                ).hexdigest(),\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "    for entity_denotation in entities_in_sentence[\"denotations\"]:\n",
    "        # The 'denotations' created above do not have an 'id' list.\n",
    "        # The 'lemma' field is intended to be the unique identifier for the graph node.\n",
    "        entity_id = entity_denotation[\"lemma\"] # Use lemma as the ID\n",
    "        other_ids = [] # No other IDs are being generated in this pipeline currently.\n",
    "        entity_type = entity_denotation[\"obj\"]\n",
    "\n",
    "        # Use the already normalized lemma from the denotation for the entity name in the graph\n",
    "        processed_lemma_for_graph = entity_denotation[\"lemma\"]\n",
    "\n",
    "        # Apply final filtering steps to this processed_lemma\n",
    "        if processed_lemma_for_graph in STOPWORDS or len(processed_lemma_for_graph) < 3 or len(processed_lemma_for_graph) > 60 or processed_lemma_for_graph.isdigit() or not re.search(r'[a-z√°√©√≠√≥√∫√±]', processed_lemma_for_graph, re.I):\n",
    "            continue\n",
    "\n",
    "        e.append(\n",
    "            {\n",
    "                \"entity_id\": entity_id, # This is the normalized lemma\n",
    "                \"other_ids\": other_ids, # This will be empty\n",
    "                \"entity_type\": entity_type,\n",
    "                \"entity\": processed_lemma_for_graph, # Use the consistent lemma for the entity name in the graph\n",
    "                \"start\": entity_denotation[\"span\"][\"begin\"], # Add start index\n",
    "                \"end\": entity_denotation[\"span\"][\"end\"]    # Add end index\n",
    "            }\n",
    "        )\n",
    "\n",
    "    parsed_entities.append(\n",
    "        {\n",
    "            \"entities\": e,\n",
    "            \"text\": entities_in_sentence[\"text\"],\n",
    "            \"text_sha256\": hashlib.sha256(entities_in_sentence[\"text\"].encode(\"utf-8\")).hexdigest(),\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-z_7dexLD20"
   },
   "source": [
    "# Construccion del Grafo de Conocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dC7MWpZ-f6-l",
    "outputId": "f5cb1299-f2d2-4963-d406-3a52594b4d38"
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "def check_dependency(token1, token2):\n",
    "    \"\"\"\n",
    "    Retorna True si hay una conexi√≥n gramatical fuerte entre dos tokens.\n",
    "    \"\"\"\n",
    "    # 1. Uno es ancestro del otro (ej: \"tumor\" -> \"cerebral\")\n",
    "    if token1 in token2.ancestors or token2 in token1.ancestors:\n",
    "        return True\n",
    "\n",
    "    # 2. Comparten el mismo padre inmediato y ese padre es VERBO o AUXILIAR\n",
    "    # Ej: \"La resecci√≥n (Sujeto) ELIMIN√ì (Verbo) el tumor (Objeto)\"\n",
    "    if token1.head == token2.head and token1.head.pos_ in [\"VERB\", \"AUX\"]:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Asumiendo que nlp, check_dependency y parsed_entities est√°n definidos previamente\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "print(\"üï∏Ô∏è Construyendo grafo basado en sintaxis...\")\n",
    "\n",
    "for item in parsed_entities:\n",
    "    sentence_text = item[\"text\"]\n",
    "    entities = item[\"entities\"]\n",
    "\n",
    "    # Analizamos la oraci√≥n con spaCy\n",
    "    doc = nlp(sentence_text)\n",
    "\n",
    "    spacy_entities = []\n",
    "\n",
    "    # 1. Alinear entidades del NER (caracteres) con Tokens de spaCy\n",
    "    for ent in entities:\n",
    "        # char_span crea un Span de spaCy basado en indices de caracteres\n",
    "        # alignment_mode=\"contract\" asegura que no rompamos palabras a la mitad\n",
    "        span = doc.char_span(ent[\"start\"], ent[\"end\"], alignment_mode=\"contract\")\n",
    "\n",
    "        if span:\n",
    "            spacy_entities.append({\n",
    "                \"span\": span,     # El objeto Span de spaCy\n",
    "                \"lemma\": ent[\"entity_id\"], # El nombre para el nodo del grafo\n",
    "                \"type\": ent[\"entity_type\"]\n",
    "            })\n",
    "\n",
    "    # 2. Evaluar pares posibles\n",
    "    if len(spacy_entities) < 2:\n",
    "        continue\n",
    "\n",
    "    pairs = combinations(spacy_entities, 2)\n",
    "\n",
    "    for e1, e2 in pairs:\n",
    "        # Obtenemos la ra√≠z sint√°ctica de la entidad (ej: en \"cancer de pulm√≥n\", la ra√≠z es \"cancer\")\n",
    "        root1 = e1[\"span\"].root\n",
    "        root2 = e2[\"span\"].root\n",
    "\n",
    "        # VERIFICACI√ìN SINT√ÅCTICA o co-ocurrencia\n",
    "        add_edge = False\n",
    "        weight_increment = 1\n",
    "\n",
    "        if check_dependency(root1, root2):\n",
    "            add_edge = True\n",
    "            weight_increment = 2  # Mayor peso para dependencias sint√°cticas\n",
    "        elif abs(root1.i - root2.i) < 5:  # Co-ocurrencia si ra√≠ces est√°n cerca (menos de 5 tokens de distancia)\n",
    "            add_edge = True\n",
    "\n",
    "        if add_edge:\n",
    "            source = e1[\"lemma\"]\n",
    "            target = e2[\"lemma\"]\n",
    "\n",
    "            # A√±adir al grafo\n",
    "            if G.has_edge(source, target):\n",
    "                G[source][target][\"weight\"] += weight_increment\n",
    "            else:\n",
    "                G.add_edge(source, target, weight=weight_increment)\n",
    "\n",
    "print(f\"‚úÖ Grafo construido: {G.number_of_nodes()} nodos y {G.number_of_edges()} conexiones.\")\n",
    "\n",
    "\n",
    "print(f\"Grafo original: {G.number_of_nodes()} nodos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 994
    },
    "id": "gjEqp7IBL67J",
    "outputId": "b23ded0d-61ab-4a74-f2e5-f9bc9f815dd4"
   },
   "outputs": [],
   "source": [
    "betweenness = nx.betweenness_centrality(G)\n",
    "\n",
    "# 1. Eliminar nodos aislados o con muy pocas conexiones (Ruido)\n",
    "betweenness_values = list(betweenness.values())\n",
    "percentile_10 = np.percentile(betweenness_values, 10)  # Percentil 10\n",
    "percentile_25 = np.percentile(betweenness_values, 25)  # Percentil 25\n",
    "\n",
    "nodes_to_remove = []\n",
    "for node in G:\n",
    "    degree = G.degree(node)\n",
    "    bc = betweenness[node]\n",
    "\n",
    "    # Eliminar solo si: grado BAJO Y betweenness BAJO\n",
    "    if degree < 2 and bc < percentile_25:\n",
    "        nodes_to_remove.append(node)\n",
    "\n",
    "G.remove_nodes_from(set(nodes_to_remove))\n",
    "\n",
    "print(f\"Grafo filtrado ahora tiene {G.number_of_nodes()} nodos.\")\n",
    "\n",
    "# 2. Guardar GEXF para Gephi\n",
    "nx.write_gexf(G, \"knowledge_graph_syntax.gexf\")\n",
    "print(\"üíæ Guardado archivo 'knowledge_graph_syntax.gexf'.\")\n",
    "\n",
    "# 3. Guardar lista de nodos\n",
    "with open(\"nodos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for node in G.nodes():\n",
    "        f.write(str(node) + \"\\n\")\n",
    "\n",
    "# 4. Visualizaci√≥n R√°pida\n",
    "if G.number_of_nodes() > 0:\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    # Layout spring suele funcionar bien para visualizar clusters\n",
    "    pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "\n",
    "    # Tama√±o de nodos basado en grado\n",
    "    d = dict(G.degree)\n",
    "    node_sizes = [v * 20 for v in d.values()]\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"#6495ED\", alpha=0.7)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "    # Solo ponemos etiquetas a los nodos m√°s importantes para no saturar\n",
    "    labels = {n: n for n in G.nodes()}\n",
    "    nx.draw_networkx_labels(G, pos, labels=labels, font_size=8, font_color=\"black\")\n",
    "\n",
    "    plt.title(\"Grafo de Conocimiento (Filtrado por Sintaxis)\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"grafo_sintactico.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è El grafo qued√≥ vac√≠o despu√©s del filtrado. Intenta bajar la exigencia del score NER o el grado m√≠nimo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_5Sr0prlI7N"
   },
   "outputs": [],
   "source": [
    "with open(\"nodos.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for node in G.nodes():\n",
    "        f.write(str(node) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0xgkQpcdsKb"
   },
   "outputs": [],
   "source": [
    "ontologies_map = {\n",
    "    \"HPO (Phenotypes)\": \"http://purl.obolibrary.org/obo/hp.owl\",\n",
    "    \"DoCS\": \"http://purl.obolibrary.org/obo/doid/translations/doid-es.owl\",\n",
    "    \"NCIT (Cancer/Bio)\": \"http://purl.obolibrary.org/obo/ncit.owl\",\n",
    "    \"HPO (Espa√±ol)\": \"http://purl.obolibrary.org/obo/hp/hp-international.owl\",\n",
    "    \"DOID (Enfermedades - Espa√±ol)\": \"http://purl.obolibrary.org/obo/doid/translations/doid-es.owl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fY8Ye28jBV_C"
   },
   "source": [
    "# Validacion del grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64cqwfIyBbk-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import networkx as nx\n",
    "from difflib import get_close_matches\n",
    "from owlready2 import get_ontology, World\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# CLASE 1: OntologyLoader\n",
    "# ==========================================\n",
    "class OntologyLoader:\n",
    "    def __init__(self, cache_dir=\"./cache_ontologies\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        if not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir)\n",
    "        self.world = World()\n",
    "\n",
    "    def load_from_url(self, url, filename=None):\n",
    "        \"\"\"Descarga y carga una ontolog√≠a desde una URL, usando cach√© local.\"\"\"\n",
    "        if not filename:\n",
    "            filename = url.split('/')[-1]\n",
    "            if not filename.endswith('.owl') and not filename.endswith('.ttl'):\n",
    "                filename += '.owl'\n",
    "\n",
    "        local_path = os.path.join(self.cache_dir, filename)\n",
    "\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"‚¨áÔ∏è Descargando ontolog√≠a desde {url}...\")\n",
    "            try:\n",
    "                response = requests.get(url, timeout=60, stream=True)\n",
    "                response.raise_for_status()\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        f.write(chunk)\n",
    "                print(f\"‚úÖ Guardado en {local_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error descargando ontolog√≠a: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"üìÇ Cargando ontolog√≠a desde cach√©: {local_path}\")\n",
    "\n",
    "        try:\n",
    "            onto = self.world.get_ontology(local_path).load()\n",
    "            return onto\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error cargando ontolog√≠a con owlready2: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_multiple(self, ontologies_map):\n",
    "        loaded_ontologies = {}\n",
    "        for name, url in ontologies_map.items():\n",
    "            print(f\"Procesando {name} : {url}\")\n",
    "            filename = url.split('/')[-1]\n",
    "            if not filename.endswith('.owl') and not filename.endswith('.ttl'):\n",
    "                filename += '.owl'\n",
    "            onto = self.load_from_url(url, filename=filename)\n",
    "            if onto:\n",
    "                loaded_ontologies[name] = onto\n",
    "            else:\n",
    "                print(f\"‚ùå No se pudo cargar la ontolog√≠a {name}\")\n",
    "        return loaded_ontologies\n",
    "\n",
    "    def get_term_labels(self, ontology):\n",
    "        labels = set()\n",
    "        # Patr√≥n para detectar IDs tipo HP_12345, GO_12345, UBERON_12345\n",
    "        id_pattern = re.compile(r'^[a-zA-Z]+_\\d+$')\n",
    "\n",
    "        if not ontology:\n",
    "            return labels\n",
    "\n",
    "        for c in ontology.classes():\n",
    "            # Solo agregamos c.name si NO parece un ID\n",
    "            if not id_pattern.match(c.name):\n",
    "                labels.add(c.name.lower())\n",
    "\n",
    "            # Agregamos etiquetas en ingl√©s\n",
    "            if hasattr(c, 'label'):\n",
    "                 for l in c.label:\n",
    "                    if hasattr(l, 'lang') and l.lang == 'en':\n",
    "                         labels.add(str(l).lower())\n",
    "                    elif not hasattr(l, 'lang'): # Fallback para strings sin idioma\n",
    "                         labels.add(str(l).lower())\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def get_relationships(self, ontology):\n",
    "        \"\"\"Extrae relaciones v√°lidas (object properties) de la ontolog√≠a.\"\"\"\n",
    "        relationships = set()\n",
    "        if not ontology:\n",
    "            return relationships\n",
    "\n",
    "        for prop in ontology.object_properties():\n",
    "            relationships.add(prop.name.lower())\n",
    "            if hasattr(prop, 'label'):\n",
    "                for l in prop.label:\n",
    "                    relationships.add(str(l).lower())\n",
    "        return relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAD3WOHHLD21"
   },
   "outputs": [],
   "source": [
    "class GraphValidator:\n",
    "    def __init__(self, loaded_ontologies=None):  # CAMBIO: Cambia ontology_urls por loaded_ontologies (dict)\n",
    "        self.loader = OntologyLoader()  # CAMBIO: Instancia el loader una vez, pero ya no lo usas para cargar aqu√≠\n",
    "        self.known_terms = set()\n",
    "        self.known_relationships = set()\n",
    "        self.ontologies = []\n",
    "        self.ontology_labels = {}  # NUEVO: para mapear t√©rminos a ontolog√≠as\n",
    "        if loaded_ontologies:\n",
    "            for name, onto in loaded_ontologies.items():  # CAMBIO: Itera sobre ontolog√≠as pre-cargadas\n",
    "                if onto:\n",
    "                    self.ontologies.append(onto)\n",
    "                    print(\"‚öôÔ∏è Indexando t√©rminos y relaciones...\", end=\"\\r\")\n",
    "                    onto_labels = self.loader.get_term_labels(onto)\n",
    "                    self.known_terms.update(onto_labels)\n",
    "                    self.known_relationships.update(self.loader.get_relationships(onto))\n",
    "                    # NUEVO: Guardar qu√© t√©rminos pertenecen a esta ontolog√≠a\n",
    "                    onto_name = name  # CAMBIO: Usa el name del dict en lugar de extraer de URL\n",
    "                    for term in onto_labels:\n",
    "                        if term not in self.ontology_labels:\n",
    "                            self.ontology_labels[term] = set()\n",
    "                        self.ontology_labels[term].add(onto_name)\n",
    "                    print(f\"Estos son los known terms: {self.known_terms}\")\n",
    "                    print(f\"‚úÖ {len(self.known_terms)} t√©rminos, {len(self.known_relationships)} relaciones indexadas.\")\n",
    "\n",
    "    def validate_term(self, term):\n",
    "        \"\"\"Valida si un t√©rmino existe (match exacto o difuso).\"\"\"\n",
    "        term_lower = term.lower()\n",
    "\n",
    "        if term_lower in self.known_terms:\n",
    "            return {\"status\": \"valid\", \"match\": term_lower, \"type\": \"exact\"}\n",
    "\n",
    "        matches = get_close_matches(term_lower, list(self.known_terms), n=1, cutoff=0.85)\n",
    "        if matches:\n",
    "            return {\"status\": \"valid\", \"match\": matches[0], \"type\": \"fuzzy\"}\n",
    "\n",
    "        return {\"status\": \"invalid\", \"match\": None, \"type\": \"none\"}\n",
    "\n",
    "    def validate_term_with_ontology(self, term):\n",
    "        \"\"\"Valida t√©rmino y devuelve en qu√© ontolog√≠as aparece.\"\"\"\n",
    "        term_lower = term.lower()\n",
    "        matching_ontologies = set()\n",
    "\n",
    "        if term_lower in self.known_terms:\n",
    "            # NUEVO: Obtener las ontolog√≠as donde aparece este t√©rmino\n",
    "            matching_ontologies = self.ontology_labels.get(term_lower, set())\n",
    "            return {\n",
    "                \"status\": \"valid\",\n",
    "                \"match\": term_lower,\n",
    "                \"type\": \"exact\",\n",
    "                \"ontologies\": list(matching_ontologies)  # NUEVO\n",
    "            }\n",
    "\n",
    "        # Para matches difusos, tambi√©n necesitamos encontrar las ontolog√≠as\n",
    "        matches = get_close_matches(term_lower, list(self.known_terms), n=1, cutoff=0.85)\n",
    "        if matches:\n",
    "            matched_term = matches[0]\n",
    "            matching_ontologies = self.ontology_labels.get(matched_term, set())\n",
    "            return {\n",
    "                \"status\": \"valid\",\n",
    "                \"match\": matched_term,\n",
    "                \"type\": \"fuzzy\",\n",
    "                \"ontologies\": list(matching_ontologies)  # NUEVO\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            \"status\": \"invalid\",\n",
    "            \"match\": None,\n",
    "            \"type\": \"none\",\n",
    "            \"ontologies\": []  # NUEVO\n",
    "        }\n",
    "\n",
    "    def validate_edge(self, source, target, edge_type):\n",
    "        \"\"\"Valida una arista: nodos + tipo de relaci√≥n.\"\"\"\n",
    "        source_validation = self.validate_term(str(source))\n",
    "        target_validation = self.validate_term(str(target))\n",
    "\n",
    "        edge_type_lower = str(edge_type).lower()\n",
    "        edge_valid = edge_type_lower in self.known_relationships\n",
    "\n",
    "        if not edge_valid:\n",
    "            matches = get_close_matches(edge_type_lower, list(self.known_relationships), n=1, cutoff=0.85)\n",
    "            if matches:\n",
    "                edge_valid = True\n",
    "                edge_match = matches[0]\n",
    "            else:\n",
    "                edge_match = None\n",
    "        else:\n",
    "            edge_match = edge_type_lower\n",
    "\n",
    "        return {\n",
    "            \"source\": source_validation,\n",
    "            \"target\": target_validation,\n",
    "            \"edge_type\": {\n",
    "                \"status\": \"valid\" if edge_valid else \"invalid\",\n",
    "                \"match\": edge_match,\n",
    "                \"original\": edge_type\n",
    "            },\n",
    "            \"fully_valid\": (\n",
    "                source_validation[\"status\"] == \"valid\" and\n",
    "                target_validation[\"status\"] == \"valid\" and\n",
    "                edge_valid\n",
    "            )\n",
    "        }\n",
    "\n",
    "\n",
    "    def validate_graph(self, graph):\n",
    "        \"\"\"Valida nodos Y aristas del grafo NetworkX.\"\"\"\n",
    "        report = {\n",
    "            \"total_nodes\": graph.number_of_nodes(),\n",
    "            \"valid_nodes\": 0,\n",
    "            \"invalid_nodes\": 0,\n",
    "            \"node_details\": {},\n",
    "            \"edge_report\": { # Nuevo: Reporte de aristas consolidado\n",
    "                \"total_edges\": graph.number_of_edges(),\n",
    "                \"valid_rels\": 0, # Aristas completamente v√°lidas\n",
    "                \"weak_rels\": 0, # Aristas inv√°lidas (sin match completo)\n",
    "                \"avg_distance\": 0.0, # No calculado actualmente, placeholder\n",
    "                \"details\": {} # Detalles individuales de cada arista\n",
    "            },\n",
    "            \"ontology_coverage\": {  # NUEVO: m√©tricas de cobertura\n",
    "                \"total_unique_terms_matched\": 0,\n",
    "                \"terms_by_ontology_count\": {},\n",
    "                \"term_ontology_distribution\": {}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # NUEVO: Para calcular cobertura entre ontolog√≠as\n",
    "        all_matched_terms = set()\n",
    "        ontology_term_counts = {}\n",
    "        term_ontology_map = {}  # Mapa t√©rmino -> [ontolog√≠as]\n",
    "\n",
    "        print(f\"\\nüîç Validando {graph.number_of_nodes()} nodos...\")\n",
    "        for node in graph.nodes():\n",
    "            result = self.validate_term_with_ontology(str(node))\n",
    "            report[\"node_details\"][node] = result\n",
    "\n",
    "            # NUEVO: Acumular estad√≠sticas de cobertura\n",
    "            if result[\"status\"] == \"valid\":\n",
    "                report[\"valid_nodes\"] += 1\n",
    "                all_matched_terms.add(result[\"match\"])\n",
    "\n",
    "                # Contar por ontolog√≠a\n",
    "                for ontology in result[\"ontologies\"]:\n",
    "                    ontology_term_counts[ontology] = ontology_term_counts.get(ontology, 0) + 1\n",
    "\n",
    "                # Mapear t√©rmino a ontolog√≠as\n",
    "                if result[\"match\"] not in term_ontology_map:\n",
    "                    term_ontology_map[result[\"match\"]] = set()\n",
    "                term_ontology_map[result[\"match\"]].update(result[\"ontologies\"])\n",
    "            else:\n",
    "                report[\"invalid_nodes\"] += 1\n",
    "\n",
    "        # NUEVO: Calcular m√©tricas de cobertura\n",
    "        report[\"ontology_coverage\"][\"total_unique_terms_matched\"] = len(all_matched_terms)\n",
    "        report[\"ontology_coverage\"][\"terms_by_ontology_count\"] = ontology_term_counts\n",
    "        report[\"ontology_coverage\"][\"term_ontology_distribution\"] = {\n",
    "            term: list(ontologies)\n",
    "            for term, ontologies in term_ontology_map.items()\n",
    "        }\n",
    "\n",
    "        # Resto del c√≥digo original para aristas...\n",
    "        print(f\"üîó Validando {graph.number_of_edges()} aristas...\")\n",
    "        for i, (source, target, data) in enumerate(graph.edges(data=True)):\n",
    "            edge_type = data.get('relation', data.get('type', 'unknown')) # Prefer 'relation' from Mistral, fallback to 'type'\n",
    "            result = self.validate_edge(source, target, edge_type)\n",
    "            edge_key = (source, target, edge_type)\n",
    "            report[\"edge_report\"][\"details\"][edge_key] = result # Almacenar en el nuevo sub-diccionario\n",
    "\n",
    "            if result[\"fully_valid\"]:\n",
    "                report[\"edge_report\"][\"valid_rels\"] += 1 # Actualizar contador en sub-diccionario\n",
    "            else:\n",
    "                report[\"edge_report\"][\"weak_rels\"] += 1 # Actualizar contador en sub-diccionario\n",
    "\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"   Procesadas {i + 1}/{report['edge_report']['total_edges']} aristas...\", end=\"\\r\")\n",
    "\n",
    "        print()  # Nueva l√≠nea\n",
    "\n",
    "        if report[\"total_nodes\"] > 0:\n",
    "            report[\"node_precision\"] = report[\"valid_nodes\"] / report[\"total_nodes\"]\n",
    "        else:\n",
    "            report[\"node_precision\"] = 0.0\n",
    "\n",
    "        # Calcular la precisi√≥n de las aristas usando los nuevos contadores\n",
    "        if report[\"edge_report\"][\"total_edges\"] > 0:\n",
    "            report[\"edge_precision\"] = report[\"edge_report\"][\"valid_rels\"] / report[\"edge_report\"][\"total_edges\"]\n",
    "        else:\n",
    "            report[\"edge_precision\"] = 0.0\n",
    "\n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VND6H6HxcMIr",
    "outputId": "eafc9a72-ffe0-4bb5-9f7c-4aefafec2f08"
   },
   "outputs": [],
   "source": [
    "loader = OntologyLoader()\n",
    "loaded_ontologies = loader.load_multiple(ontologies_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k4fKYBW6pdd",
    "outputId": "947f4426-b5db-40c8-9022-eb39acdb54c2"
   },
   "outputs": [],
   "source": [
    "validator = GraphValidator(loaded_ontologies=loaded_ontologies.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Wy6v8eK0Kcy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "if 'G' not in locals():\n",
    "    print(\"‚ö†Ô∏è ¬°Alerta! La variable 'G' no est√° definida.\")\n",
    "else:\n",
    "    print(f\"üöÄ Iniciando evaluaci√≥n del grafo ({G.number_of_nodes()} nodos, {G.number_of_edges()} aristas)...\")\n",
    "    print(f\"{'Ontolog√≠a':<25} | {'Nodos V/I':<12} | {'Aristas V/I':<13} | {'Prec. Nodos':<12} | {'Prec. Aristas':<14}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    results_summary = []\n",
    "\n",
    "    # NUEVO: Variables para estad√≠sticas globales\n",
    "    all_terms_in_graph = set(G.nodes())\n",
    "    all_matched_terms_global = set()\n",
    "    term_ontology_count_global = {}  # t√©rmino -> n√∫mero de ontolog√≠as donde aparece\n",
    "    ontology_overlap_stats = {}  # Para cada ontolog√≠a, cu√°ntos t√©rminos √∫nicos\n",
    "\n",
    "    for name, url in ontologies_map.items():\n",
    "        try:\n",
    "            print(f\"Validando la ontologia {name}\")\n",
    "            # CAMBIO: Pasa {name: loaded_ontologies[name]} en lugar de [url]\n",
    "            report = validator.validate_graph(G)\n",
    "\n",
    "            nodes_str = f\"{report['valid_nodes']}/{report['invalid_nodes']}\"\n",
    "            # CORRECTED: Access edge validation metrics from 'edge_report' sub-dictionary\n",
    "            edges_str = f\"{report['edge_report']['valid_rels']}/{report['edge_report']['weak_rels']}\"\n",
    "            node_prec = report['node_precision']\n",
    "            edge_prec = report['edge_precision']\n",
    "\n",
    "            print(f\"{name:<25} | {nodes_str:<12} | {edges_str:<13} | {node_prec:>10.2%} | {edge_prec:>12.2%}\")\n",
    "\n",
    "            # NUEVO: Acumular datos globales\n",
    "            ontology_overlap_stats[name] = {\n",
    "                'unique_terms_matched': report['ontology_coverage']['total_unique_terms_matched'],\n",
    "                'details': report['ontology_coverage']\n",
    "            }\n",
    "\n",
    "            # Acumular t√©rminos coincidentes\n",
    "            for term, ontologies in report['ontology_coverage']['term_ontology_distribution'].items():\n",
    "                all_matched_terms_global.add(term)\n",
    "                term_ontology_count_global[term] = term_ontology_count_global.get(term, 0) + len(ontologies)\n",
    "\n",
    "            results_summary.append({\n",
    "                \"ontology\": name,\n",
    "                \"node_precision\": node_prec,\n",
    "                \"edge_precision\": edge_prec,\n",
    "                \"details\": report\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{name:<25} | ‚ùå ERROR: {str(e)}\")\n",
    "\n",
    "    print(\"-\" * 95)\n",
    "    print(\"\\n‚úÖ Benchmark Completado.\")\n",
    "\n",
    "    # NUEVO: Estad√≠sticas de cobertura global\n",
    "    print(\"\\nüìä ESTAD√çSTICAS DE COBERTURA GLOBAL\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1. T√©rminos √∫nicos que coincidieron en total\n",
    "    print(f\"\\nüîπ T√©rminos √∫nicos que coincidieron en AL MENOS una ontolog√≠a: {len(all_matched_terms_global)}\")\n",
    "\n",
    "    # 2. T√©rminos que NO aparecieron en NINGUNA ontolog√≠a\n",
    "    terms_not_found = all_terms_in_graph - all_matched_terms_global\n",
    "    print(f\"üîπ T√©rminos NO encontrados en NINGUNA ontolog√≠a: {len(terms_not_found)}\")\n",
    "\n",
    "    if terms_not_found:\n",
    "        print(\"   Ejemplos (primeros 10):\")\n",
    "        for i, term in enumerate(list(terms_not_found)[:10]):\n",
    "            print(f\"   {i+1}. {term}\")\n",
    "\n",
    "    # 3. Top 10 t√©rminos con m√°s apariciones en ontolog√≠as\n",
    "    print(f\"\\nüèÜ TOP 10 t√©rminos con m√°s apariciones en ontolog√≠as:\")\n",
    "    sorted_terms = sorted(term_ontology_count_global.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for i, (term, count) in enumerate(sorted_terms, 1):\n",
    "        # Obtener en qu√© ontolog√≠as espec√≠ficamente aparece\n",
    "        ontologies_for_term = []\n",
    "        for result in results_summary:\n",
    "            if term in result['details']['ontology_coverage']['term_ontology_distribution']:\n",
    "                ontologies_for_term.append(result['ontology'])\n",
    "\n",
    "        print(f\"   {i}. {term} ({count} ontolog√≠as)\")\n",
    "        print(f\"      Aparece en: {', '.join(ontologies_for_term[:3])}\" +\n",
    "              (\"...\" if len(ontologies_for_term) > 3 else \"\"))\n",
    "\n",
    "    # 4. Distribuci√≥n por ontolog√≠a\n",
    "    print(f\"\\nüìà DISTRIBUCI√ìN DE T√âRMINOS POR ONTOLOG√çA:\")\n",
    "    for ontology_name, stats in ontology_overlap_stats.items():\n",
    "        print(f\"   {ontology_name}: {stats['unique_terms_matched']} t√©rminos √∫nicos\")\n",
    "\n",
    "    # 5. Solapamiento entre ontolog√≠as (opcional)\n",
    "    if len(ontology_overlap_stats) > 1:\n",
    "        print(f\"\\nüîÑ SOLAPAMIENTO ENTRE ONTOLOG√çAS:\")\n",
    "        ontologies_list = list(ontology_overlap_stats.keys())\n",
    "        for i in range(len(ontologies_list)):\n",
    "            for j in range(i+1, len(ontologies_list)):\n",
    "                onto1 = ontologies_list[i]\n",
    "                onto2 = ontologies_list[j]\n",
    "\n",
    "                # T√©rminos de cada ontolog√≠a (simplificado)\n",
    "                terms1 = set(results_summary[i]['details']['ontology_coverage']['term_ontology_distribution'].keys())\n",
    "                terms2 = set(results_summary[j]['details']['ontology_coverage']['term_ontology_distribution'].keys())\n",
    "\n",
    "                common_terms = terms1.intersection(terms2)\n",
    "                if common_terms:\n",
    "                    print(f\"   {onto1} ‚Üî {onto2}: {len(common_terms)} t√©rminos en com√∫n\")\n",
    "\n",
    "    if results_summary:\n",
    "        best_result = max(results_summary, key=lambda x: (x['node_precision'] + x['edge_precision']) / 2)\n",
    "        print(f\"\\nüîé Mejor ontolog√≠a: {best_result['ontology']}\")\n",
    "        print(f\"   Precisi√≥n nodos: {best_result['node_precision']:.2%}\")\n",
    "        print(f\"   Precisi√≥n aristas: {best_result['edge_precision']:.2%}\")\n",
    "\n",
    "        # CORRECTED: Access edge_details from 'edge_report' sub-dictionary\n",
    "        invalid_edges = [(k, v) for k, v in best_result['details']['edge_report']['details'].items() if not v['fully_valid']]\n",
    "        if invalid_edges:\n",
    "            print(f\"\\n‚ö†Ô∏è Ejemplos de aristas inv√°lidas (primeras 5):\")\n",
    "            for (s, t, et), details in invalid_edges[:5]:\n",
    "                print(f\"   {s} --[{et}]--> {t}\")\n",
    "                if details['edge_type']['status'] == 'invalid':\n",
    "                    print(f\"      ‚îî‚îÄ Tipo de arista '{et}' no encontrado en ontolog√≠a\")\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMmKt095rYnr"
   },
   "source": [
    "# Aristas faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PmfB3jZMLoIu"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from mistralai import Mistral\n",
    "import json\n",
    "import time\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.graph= nx.Graph()\n",
    "        self.cache_file = \"cache_etiquetas.json\"\n",
    "        self.cache = self._cargar_cache()\n",
    "\n",
    "    def add_edge(self, source, target, relation='cooccurs_with'):\n",
    "        if self.graph.has_edge(source, target):\n",
    "            self.graph[source][target][\"weight\"] += 1\n",
    "        else:\n",
    "            self.graph.add_edge(source, target, weight=1, relation=relation)\n",
    "\n",
    "    def build_relations(self, entity_names):\n",
    "        pairs = combinations(entity_names, 2)\n",
    "\n",
    "        for source, target in pairs:\n",
    "            self.add_edge(source, target, relation='cooccurs_with')\n",
    "\n",
    "    def build_graph(self, parsed_entities):\n",
    "        for item in parsed_entities:\n",
    "            if \"entities\" in item:\n",
    "                entity_names = [e[\"entity\"].lower().strip() for e in item[\"entities\"]]\n",
    "                entity_names = list(set(entity_names))\n",
    "\n",
    "                # Si hay menos de 2 entidades, no podemos hacer una conexi√≥n\n",
    "                if len(entity_names) < 2:\n",
    "                    continue\n",
    "\n",
    "                self.build_relations(entity_names)\n",
    "\n",
    "\n",
    "    def _cargar_cache(self):\n",
    "        \"\"\"Carga el archivo JSON de cach√© si existe.\"\"\"\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r', encoding='utf-8') as f:\n",
    "                    return json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"Aviso: No se pudo leer el cach√© ({e}). Se iniciar√° uno nuevo.\")\n",
    "                return {}\n",
    "        return {}\n",
    "\n",
    "    def _guardar_cache(self):\n",
    "        \"\"\"Guarda el estado actual del cach√© en el archivo JSON.\"\"\"\n",
    "        with open(self.cache_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.cache, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def _obtener_key(self, u, v):\n",
    "        \"\"\"Genera una clave √∫nica para identificar la arista.\"\"\"\n",
    "        return f\"{u}-->{v}\"\n",
    "\n",
    "    def analyze_topology(self):\n",
    "        print(\"\\nüï∏Ô∏è An√°lisis Topol√≥gico:\")\n",
    "        n = self.graph.number_of_nodes()\n",
    "        e = self.graph.number_of_edges()\n",
    "\n",
    "        # 1. Densidad\n",
    "        density = nx.density(self.graph)\n",
    "        print(f\"   ‚Ä¢ Densidad: {density:.4f}\")\n",
    "\n",
    "        # 2. Componentes Conectados (importante para saber si hay islas)\n",
    "        if nx.is_directed(self.graph):\n",
    "            n_components = nx.number_weakly_connected_components(self.graph)\n",
    "        else:\n",
    "            n_components = nx.number_connected_components(self.graph)\n",
    "        print(f\"   ‚Ä¢ Componentes Conectados: {n_components}\")\n",
    "\n",
    "        # 3. Grado Promedio\n",
    "        avg_degree = sum(dict(self.graph.degree()).values()) / n\n",
    "        print(f\"   ‚Ä¢ Grado Promedio: {avg_degree:.2f}\")\n",
    "\n",
    "        # 4. Clustering (solo para no dirigidos o convertir primero)\n",
    "        # Indica si los vecinos de un nodo tambi√©n son vecinos entre s√≠ (com√∫n en biolog√≠a)\n",
    "        try:\n",
    "            avg_clustering = nx.average_clustering(self.graph.to_undirected())\n",
    "            print(f\"   ‚Ä¢ Coeficiente de Clustering: {avg_clustering:.4f}\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return {\"density\": density, \"components\": n_components}\n",
    "\n",
    "    def label_edges(self, api_key, use_cache=True, batch_size=50):\n",
    "        \"\"\"\n",
    "        Consulta etiquetas a Mistral y las persiste en cach√©.\n",
    "        \"\"\"\n",
    "        aristas_totales = list(self.graph.edges())\n",
    "        aristas_a_consultar = []\n",
    "        G_etiquetado = nx.DiGraph()\n",
    "\n",
    "        # 1. Identificar qu√© necesitamos consultar y qu√© ya tenemos\n",
    "        for u, v in aristas_totales:\n",
    "            key = self._obtener_key(u, v)\n",
    "            if use_cache and key in self.cache:\n",
    "                # Si usamos cach√© y existe, lo a√±adimos directamente\n",
    "                G_etiquetado.add_edge(u, v, label=self.cache[key])\n",
    "            else:\n",
    "                # Si no usamos cach√© o no existe la arista, hay que preguntar a Mistral\n",
    "                aristas_a_consultar.append((u, v))\n",
    "\n",
    "        if not aristas_a_consultar:\n",
    "            print(\"Resultado obtenido √≠ntegramente desde el cach√©.\")\n",
    "            return G_etiquetado\n",
    "\n",
    "        print(f\"Total: {len(aristas_totales)} | Desde Cach√©: {len(aristas_totales) - len(aristas_a_consultar)} | Pendientes: {len(aristas_a_consultar)}\")\n",
    "\n",
    "        # 2. Procesamiento por lotes (Batching)\n",
    "        client = Mistral(api_key=api_key)\n",
    "        for i in range(0, len(aristas_a_consultar), batch_size):\n",
    "            batch = aristas_a_consultar[i : i + batch_size]\n",
    "            num_lote = i // batch_size + 1\n",
    "            print(f\"Procesando lote {num_lote} de {((len(aristas_a_consultar)-1)//batch_size)+1}...\")\n",
    "\n",
    "            prompt_sistema = (\n",
    "                \"Eres un experto en ontolog√≠as. Devuelve un JSON con una etiqueta sem√°ntica \"\n",
    "                \"corta para cada relaci√≥n [A, B]. Formato: {'relaciones': [[A, B, 'etiqueta'], ...]}\"\n",
    "            )\n",
    "            prompt_usuario = f\"Etiqueta estas aristas: {json.dumps(batch)}\"\n",
    "\n",
    "            try:\n",
    "                chat_response = client.chat.complete(\n",
    "                    model=\"mistral-small-latest\",\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": prompt_sistema},\n",
    "                        {\"role\": \"user\", \"content\": prompt_usuario},\n",
    "                    ],\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "\n",
    "                datos_lote = json.loads(chat_response.choices[0].message.content)\n",
    "\n",
    "                # Procesar resultados del lote\n",
    "                for item in datos_lote.get(\"relaciones\", []):\n",
    "                    if len(item) == 3:\n",
    "                        u_res, v_res, etiqueta = item\n",
    "\n",
    "                        # CORRECCI√ìN: Calcular key_res ANTES de usarlo\n",
    "                        key_res = self._obtener_key(u_res, v_res)\n",
    "\n",
    "                        # CORRECCI√ìN: A√±adir al grafo final usando u_res y v_res (del item)\n",
    "                        G_etiquetado.add_edge(u_res, v_res, relation=etiqueta)\n",
    "\n",
    "                        # Actualizar cach√©\n",
    "                        self.cache[key_res] = etiqueta\n",
    "\n",
    "                # Persistencia inmediata tras cada lote exitoso\n",
    "                self._guardar_cache()\n",
    "\n",
    "                # Peque√±a pausa para respetar l√≠mites de la API\n",
    "                if i + batch_size < len(aristas_a_consultar):\n",
    "                    time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error en el lote {num_lote}: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"Proceso finalizado. Grafo resultante con {G_etiquetado.number_of_edges()} aristas.\")\n",
    "        return G_etiquetado\n",
    "\n",
    "    def run(self, parsed_entities):\n",
    "        self.build_graph(parsed_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_T3v5pUG_eta",
    "outputId": "591daad8-8077-483c-b669-965a2e3ca402"
   },
   "outputs": [],
   "source": [
    "G_original = G.copy()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 1: Instalar PyTorch Geometric (si no est√°)\n",
    "# -------------------------------------------------\n",
    "try:\n",
    "    import torch\n",
    "    from torch_geometric.data import Data\n",
    "    print(\"‚úÖ PyTorch y PyG ya instalados\")\n",
    "except ImportError:\n",
    "    print(\"Instalando PyTorch Geometric...\")\n",
    "    !pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
    "    import torch\n",
    "    from torch_geometric.data import Data\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import numpy as np\n",
    "\n",
    "# === Focal Loss ===\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 2: Convertir NetworkX a PyTorch Geometric\n",
    "# -------------------------------------------------\n",
    "def prepare_gcn_data(G):\n",
    "    \"\"\"Convierte tu grafo NetworkX a formato PyG\"\"\"\n",
    "    print(\"üìä Preparando datos para GCN...\")\n",
    "\n",
    "    # Mapeo de nodos a √≠ndices\n",
    "    node_list = list(G.nodes())\n",
    "    node_to_idx = {node: i for i, node in enumerate(node_list)}\n",
    "    idx_to_node = {i: node for node, i in node_to_idx.items()}\n",
    "\n",
    "    # Features de nodos (inicialmente aleatorios, podr√≠as usar embeddings de BERT despu√©s)\n",
    "    num_nodes = len(node_list)\n",
    "    node_features = torch.randn((num_nodes, 128))\n",
    "\n",
    "    # Edge index (formato [2, num_edges])\n",
    "    edge_index = []\n",
    "    for u, v in G.edges():\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    # Crear objeto Data de PyG\n",
    "    pyg_data = Data(\n",
    "        x=node_features,\n",
    "        edge_index=edge_index,\n",
    "        num_nodes=num_nodes\n",
    "    )\n",
    "\n",
    "    print(f\"  ‚Ä¢ Nodos convertidos: {pyg_data.num_nodes}\")\n",
    "    print(f\"  ‚Ä¢ Aristas convertidas: {pyg_data.edge_index.shape[1]}\")\n",
    "\n",
    "    return pyg_data, node_to_idx, idx_to_node\n",
    "\n",
    "# Convertir tu grafo G\n",
    "pyg_data, node_to_idx, idx_to_node = prepare_gcn_data(G)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 3: Definir Modelo GCN para Link Prediction\n",
    "# -------------------------------------------------\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class GCNLinkPredictor(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        \"\"\"Genera embeddings de nodos\"\"\"\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        \"\"\"Predice si existe arista entre pares de nodos\"\"\"\n",
    "        src = z[edge_label_index[0]]\n",
    "        dst = z[edge_label_index[1]]\n",
    "        return (src * dst).sum(dim=-1)  # Similaridad por producto punto\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 4: Dividir Aristas para Entrenamiento\n",
    "# -------------------------------------------------\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "print(\"\\nüîÄ Dividiendo aristas para entrenamiento/validaci√≥n/test...\")\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.15,    # 15% validaci√≥n\n",
    "    num_test=0.15,   # 15% test\n",
    "    is_undirected=True,\n",
    "    add_negative_train_samples=False,\n",
    "    neg_sampling_ratio=1.0  # 1 negativo por cada positivo\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(pyg_data)\n",
    "\n",
    "print(f\"  ‚Ä¢ Aristas entrenamiento: {train_data.edge_label_index.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Aristas validaci√≥n: {val_data.edge_label_index.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Aristas test: {test_data.edge_label_index.shape[1]}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 5: Entrenar el Modelo (actualizado con m√©tricas)\n",
    "# -------------------------------------------------\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(predictions, labels, threshold=0.5):\n",
    "    \"\"\"Calcula precision, recall, f1 y curva PRC\"\"\"\n",
    "    # Convertir probabilidades a predicciones binarias\n",
    "    binary_preds = (predictions >= threshold).astype(int)\n",
    "\n",
    "    # M√©tricas b√°sicas\n",
    "    precision = precision_score(labels, binary_preds, zero_division=0)\n",
    "    recall = recall_score(labels, binary_preds, zero_division=0)\n",
    "    f1 = f1_score(labels, binary_preds, zero_division=0)\n",
    "\n",
    "    # Curva Precision-Recall\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(labels, predictions)\n",
    "    prc_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'prc_auc': prc_auc,\n",
    "        'precision_curve': precision_vals,\n",
    "        'recall_curve': recall_vals,\n",
    "        'thresholds': thresholds\n",
    "    }\n",
    "\n",
    "def train_gcn_model_with_metrics():\n",
    "    \"\"\"Entrenamiento con hard negatives + focal loss\"\"\"\n",
    "    print(\"\\nüéØ Entrenando modelo GCN con HARD NEGATIVES + FOCAL LOSS...\")\n",
    "\n",
    "    # Hiperpar√°metros (puedes ajustar gamma y percentile)\n",
    "    EPOCHS = 100\n",
    "    LEARNING_RATE = 0.01\n",
    "    HIDDEN_DIM = 256\n",
    "    GAMMA_FOCAL = 2.0          # t√≠pico: 2.0‚Äì3.0\n",
    "    ALPHA_FOCAL = 1.0\n",
    "    DEGREE_PERCENTILE = 70     # nodos con grado > percentil 70 se consideran \"high degree\"\n",
    "\n",
    "    model = GCNLinkPredictor(\n",
    "        in_channels=128,\n",
    "        hidden_channels=HIDDEN_DIM,\n",
    "        out_channels=128\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = FocalLoss(alpha=ALPHA_FOCAL, gamma=GAMMA_FOCAL)\n",
    "\n",
    "    # === Pre-calcular nodos de alto grado para hard negatives ===\n",
    "    degrees = [G_original.degree(idx_to_node[i]) for i in range(pyg_data.num_nodes)]\n",
    "    high_degree_threshold = np.percentile(degrees, DEGREE_PERCENTILE)\n",
    "    high_degree_nodes = torch.tensor([\n",
    "        i for i in range(pyg_data.num_nodes)\n",
    "        if degrees[i] > high_degree_threshold\n",
    "    ], dtype=torch.long)\n",
    "\n",
    "    # Si no hay suficientes nodos de alto grado, fallback a todos\n",
    "    if len(high_degree_nodes) < 100:\n",
    "        print(\"‚ö†Ô∏è Pocos nodos de alto grado ‚Üí usando todos los nodos para negativos\")\n",
    "        high_degree_nodes = torch.arange(pyg_data.num_nodes)\n",
    "\n",
    "    # Probabilidades sesgadas hacia nodos de alto grado\n",
    "    prob = torch.zeros(pyg_data.num_nodes)\n",
    "    prob[high_degree_nodes] = 1.0\n",
    "    prob = prob / prob.sum()\n",
    "    sampler = dist.Categorical(prob)\n",
    "\n",
    "    best_prc_auc = 0\n",
    "    best_model_state = None\n",
    "    best_metrics = {}\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # === HARD NEGATIVE SAMPLING ===\n",
    "        num_neg_samples = train_data.edge_label_index.size(1)\n",
    "        neg_src = sampler.sample((num_neg_samples,))\n",
    "        neg_dst = sampler.sample((num_neg_samples,))\n",
    "        neg_edge_index = torch.stack([neg_src, neg_dst], dim=0)\n",
    "\n",
    "        # Asegurar que no se generen negativos que ya existan (opcional pero recomendado)\n",
    "        # Esto es ligero porque el grafo es sparse\n",
    "        full_edge_index = torch.cat([train_data.edge_index, neg_edge_index], dim=1)\n",
    "        # (Puedes a√±adir un filtro m√°s estricto si quieres)\n",
    "\n",
    "        # Combinar positivas y negativas\n",
    "        all_edge_index = torch.cat([\n",
    "            train_data.edge_label_index,\n",
    "            neg_edge_index\n",
    "        ], dim=1)\n",
    "        all_edge_labels = torch.cat([\n",
    "            torch.ones(train_data.edge_label_index.size(1)),\n",
    "            torch.zeros(neg_edge_index.size(1))\n",
    "        ], dim=0).to(train_data.x.device)\n",
    "\n",
    "        # Forward pass\n",
    "        z = model.encode(train_data.x, train_data.edge_index)\n",
    "        out = model.decode(z, all_edge_index)\n",
    "        loss = criterion(out, all_edge_labels)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validaci√≥n cada 10 √©pocas\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                z_val = model.encode(val_data.x, val_data.edge_index)\n",
    "                val_out = model.decode(z_val, val_data.edge_label_index)\n",
    "                val_pred = torch.sigmoid(val_out).cpu().numpy()\n",
    "                val_true = val_data.edge_label.cpu().numpy()\n",
    "\n",
    "                auc_score = roc_auc_score(val_true, val_pred)\n",
    "                metrics = calculate_metrics(val_pred, val_true)\n",
    "\n",
    "                history.append({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'loss': loss.item(),\n",
    "                    'auc': auc_score,\n",
    "                    'precision': metrics['precision'],\n",
    "                    'recall': metrics['recall'],\n",
    "                    'f1': metrics['f1'],\n",
    "                    'prc_auc': metrics['prc_auc']\n",
    "                })\n",
    "\n",
    "                if metrics['prc_auc'] > best_prc_auc:\n",
    "                    best_prc_auc = metrics['prc_auc']\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                    best_metrics = metrics.copy()\n",
    "\n",
    "                print(f' Epoch {epoch+1:03d} | Loss: {loss.item():.4f} | '\n",
    "                      f'AUC: {auc_score:.4f} | PRC-AUC: {metrics[\"prc_auc\"]:.4f} | '\n",
    "                      f'Precision: {metrics[\"precision\"]:.4f} | Recall: {metrics[\"recall\"]:.4f}')\n",
    "\n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä RESUMEN DE ENTRENAMIENTO (con hard negatives + focal loss)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\" ‚Ä¢ Mejor AUC (ROC): {history[-1]['auc']:.4f}\")\n",
    "    print(f\" ‚Ä¢ Mejor AUC (PRC): {best_prc_auc:.4f}\")\n",
    "    print(f\" ‚Ä¢ Mejor Precision: {best_metrics.get('precision', 0):.4f}\")\n",
    "    print(f\" ‚Ä¢ Mejor Recall: {best_metrics.get('recall', 0):.4f}\")\n",
    "    print(f\" ‚Ä¢ Mejor F1-Score: {best_metrics.get('f1', 0):.4f}\")\n",
    "\n",
    "    return best_model_state, best_metrics, history\n",
    "\n",
    "# Entrenar modelo con m√©tricas\n",
    "best_model_state, best_val_metrics, training_history = train_gcn_model_with_metrics()\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 6: Evaluar y Predecir Nuevas Conexiones (actualizado)\n",
    "# -------------------------------------------------\n",
    "def evaluate_model_comprehensively(model_state, threshold=0.5):\n",
    "    \"\"\"Eval√∫a el modelo con todas las m√©tricas\"\"\"\n",
    "    print(\"\\nüìà Evaluaci√≥n completa del modelo...\")\n",
    "\n",
    "    # Cargar mejor modelo\n",
    "    model = GCNLinkPredictor(128, 256, 128)\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Obtener embeddings de todos los nodos\n",
    "        z = model.encode(pyg_data.x, pyg_data.edge_index)\n",
    "\n",
    "        # Evaluar en conjunto de test\n",
    "        test_out = model.decode(z, test_data.edge_label_index)\n",
    "        test_pred_probs = torch.sigmoid(test_out).cpu().numpy()\n",
    "        test_true = test_data.edge_label.cpu().numpy()\n",
    "\n",
    "        # Calcular AUC-ROC\n",
    "        test_auc = roc_auc_score(test_true, test_pred_probs)\n",
    "\n",
    "        # Calcular todas las m√©tricas\n",
    "        test_metrics = calculate_metrics(test_pred_probs, test_true, threshold)\n",
    "\n",
    "        # Mostrar resultados\n",
    "        print(f\"  ‚Ä¢ AUC-ROC (Test): {test_auc:.4f}\")\n",
    "        print(f\"  ‚Ä¢ AUC-PRC (Test): {test_metrics['prc_auc']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Precision (Test): {test_metrics['precision']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ Recall (Test): {test_metrics['recall']:.4f}\")\n",
    "        print(f\"  ‚Ä¢ F1-Score (Test): {test_metrics['f1']:.4f}\")\n",
    "\n",
    "        # Evaluar tambi√©n en validaci√≥n para comparaci√≥n\n",
    "        val_out = model.decode(z, val_data.edge_label_index)\n",
    "        val_pred_probs = torch.sigmoid(val_out).cpu().numpy()\n",
    "        val_true = val_data.edge_label.cpu().numpy()\n",
    "        val_auc = roc_auc_score(val_true, val_pred_probs)\n",
    "        val_metrics = calculate_metrics(val_pred_probs, val_true, threshold)\n",
    "\n",
    "        print(f\"\\n  ‚Ä¢ AUC-ROC (Val): {val_auc:.4f}\")\n",
    "        print(f\"  ‚Ä¢ AUC-PRC (Val): {val_metrics['prc_auc']:.4f}\")\n",
    "\n",
    "        return z, test_auc, test_metrics, test_pred_probs, test_true\n",
    "\n",
    "def predict_new_links_with_threshold(model_state, top_k=50, score_threshold=None):\n",
    "    \"\"\"Predice nuevas conexiones con umbral adaptativo\"\"\"\n",
    "    print(f\"\\nüîÆ Prediciendo nuevas conexiones...\")\n",
    "\n",
    "    # Evaluar primero para determinar umbral √≥ptimo\n",
    "    z, test_auc, test_metrics, _, _ = evaluate_model_comprehensively(model_state)\n",
    "\n",
    "    # Determinar umbral autom√°ticamente si no se proporciona\n",
    "    if score_threshold is None:\n",
    "        # Usar umbral que maximice F1-score en validaci√≥n\n",
    "        model = GCNLinkPredictor(128, 256, 128)\n",
    "        model.load_state_dict(model_state)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            z_val = model.encode(val_data.x, val_data.edge_index)\n",
    "            val_out = model.decode(z_val, val_data.edge_label_index)\n",
    "            val_pred_probs = torch.sigmoid(val_out).cpu().numpy()\n",
    "            val_true = val_data.edge_label.cpu().numpy()\n",
    "\n",
    "            # Encontrar umbral √≥ptimo\n",
    "            f1_scores = []\n",
    "            thresholds = np.linspace(0.1, 0.9, 50)\n",
    "            for th in thresholds:\n",
    "                binary_preds = (val_pred_probs >= th).astype(int)\n",
    "                f1 = f1_score(val_true, binary_preds, zero_division=0)\n",
    "                f1_scores.append(f1)\n",
    "\n",
    "            optimal_idx = np.argmax(f1_scores)\n",
    "            score_threshold = thresholds[optimal_idx]\n",
    "            print(f\"  ‚Ä¢ Umbral √≥ptimo (max F1): {score_threshold:.4f}\")\n",
    "\n",
    "    # Predecir todas las posibles conexiones faltantes\n",
    "    num_nodes = pyg_data.num_nodes\n",
    "    predicted_edges = []\n",
    "\n",
    "    # Matriz de adyacencia para verificar conexiones existentes\n",
    "    existing_edges = set()\n",
    "    for i in range(pyg_data.edge_index.shape[1]):\n",
    "        u = pyg_data.edge_index[0, i].item()\n",
    "        v = pyg_data.edge_index[1, i].item()\n",
    "        existing_edges.add((u, v))\n",
    "        existing_edges.add((v, u))\n",
    "\n",
    "    # Calcular similitud para pares no conectados\n",
    "    print(\"  ‚Ä¢ Calculando similitudes...\")\n",
    "    batch_size = 1000\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(i + 1, num_nodes):\n",
    "            if (i, j) not in existing_edges:\n",
    "                similarity = torch.dot(z[i], z[j]).item()\n",
    "                predicted_edges.append({\n",
    "                    'source': idx_to_node[i],\n",
    "                    'target': idx_to_node[j],\n",
    "                    'score': similarity,\n",
    "                    'source_idx': i,\n",
    "                    'target_idx': j\n",
    "                })\n",
    "\n",
    "    # Filtrar por umbral y ordenar\n",
    "    predicted_edges = [e for e in predicted_edges if e['score'] >= score_threshold]\n",
    "    predicted_edges.sort(key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    # Tomar top_k o todas las que superen el umbral\n",
    "    if top_k is not None:\n",
    "        top_predictions = predicted_edges[:top_k]\n",
    "    else:\n",
    "        top_predictions = predicted_edges\n",
    "\n",
    "    print(f\"  ‚Ä¢ Predicciones sobre umbral: {len(predicted_edges)}\")\n",
    "    print(f\"  ‚Ä¢ Top predicciones: {len(top_predictions)}\")\n",
    "\n",
    "    return test_auc, test_metrics, top_predictions, z, score_threshold\n",
    "\n",
    "test_auc, test_metrics, gcn_predictions, node_embeddings, optimal_threshold = predict_new_links_with_threshold(\n",
    "    best_model_state,\n",
    "    top_k=500,\n",
    "    score_threshold=0.5\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Obteniendo probabilidades de test para diagn√≥stico...\")\n",
    "z_final, test_auc_final, test_metrics_final, test_pred_probs, test_true = evaluate_model_comprehensively(\n",
    "    best_model_state,\n",
    "    threshold=optimal_threshold\n",
    ")\n",
    "\n",
    "# Actualizar variables si es necesario\n",
    "test_auc = test_auc_final\n",
    "test_metrics = test_metrics_final\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 7: Crear Grafo Enriquecido con GCN (actualizado)\n",
    "# -------------------------------------------------\n",
    "print(\"\\nüîÑ Integrando predicciones GCN al grafo original...\")\n",
    "\n",
    "# Crear grafo enriquecido\n",
    "G_gcn = G_original.copy()\n",
    "gcn_added_edges = 0\n",
    "\n",
    "# Estad√≠sticas de scores\n",
    "scores = [p['score'] for p in gcn_predictions]\n",
    "print(f\"  ‚Ä¢ Rango de scores: [{min(scores):.4f}, {max(scores):.4f}]\")\n",
    "print(f\"  ‚Ä¢ Score promedio: {np.mean(scores):.4f}\")\n",
    "print(f\"  ‚Ä¢ Score mediana: {np.median(scores):.4f}\")\n",
    "\n",
    "# A√±adir aristas predichas\n",
    "for pred in gcn_predictions:\n",
    "    if pred['score'] >= optimal_threshold:\n",
    "        if not G_gcn.has_edge(pred['source'], pred['target']):\n",
    "            G_gcn.add_edge(\n",
    "                pred['source'],\n",
    "                pred['target'],\n",
    "                weight=float(pred['score']),\n",
    "                relation='predicted_by_GCN',\n",
    "                origin='GCN_prediction',\n",
    "                prediction_score=float(pred['score']),\n",
    "                prediction_rank=gcn_predictions.index(pred) + 1\n",
    "            )\n",
    "            gcn_added_edges += 1\n",
    "\n",
    "print(f\"  ‚Ä¢ Aristas a√±adidas por GCN: {gcn_added_edges}\")\n",
    "print(f\"  ‚Ä¢ Grafo GCN enriquecido: {G_gcn.number_of_nodes()} nodos, {G_gcn.number_of_edges()} aristas\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 8: Visualizaci√≥n de M√©tricas\n",
    "# -------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics_comparison(training_history, test_metrics, val_metrics):\n",
    "    \"\"\"Visualiza las m√©tricas de entrenamiento y evaluaci√≥n\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Extraer datos del historial\n",
    "    epochs = [h['epoch'] for h in training_history]\n",
    "    losses = [h['loss'] for h in training_history]\n",
    "    aucs = [h['auc'] for h in training_history]\n",
    "    precisions = [h['precision'] for h in training_history]\n",
    "    recalls = [h['recall'] for h in training_history]\n",
    "    f1s = [h['f1'] for h in training_history]\n",
    "\n",
    "    # 1. P√©rdida durante entrenamiento\n",
    "    axes[0, 0].plot(epochs, losses, 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('√âpoca')\n",
    "    axes[0, 0].set_ylabel('P√©rdida')\n",
    "    axes[0, 0].set_title('P√©rdida durante Entrenamiento')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. AUC durante entrenamiento\n",
    "    axes[0, 1].plot(epochs, aucs, 'g-', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('√âpoca')\n",
    "    axes[0, 1].set_ylabel('AUC-ROC')\n",
    "    axes[0, 1].set_title('AUC durante Entrenamiento')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Precision, Recall, F1 durante entrenamiento\n",
    "    axes[0, 2].plot(epochs, precisions, 'r-', label='Precision', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, recalls, 'g-', label='Recall', linewidth=2)\n",
    "    axes[0, 2].plot(epochs, f1s, 'b-', label='F1-Score', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('√âpoca')\n",
    "    axes[0, 2].set_ylabel('Valor')\n",
    "    axes[0, 2].set_title('M√©tricas durante Entrenamiento')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Curva Precision-Recall\n",
    "    axes[1, 0].plot(test_metrics['recall_curve'], test_metrics['precision_curve'],\n",
    "                     'b-', linewidth=2, label=f'Test (AUC={test_metrics[\"prc_auc\"]:.3f})')\n",
    "    axes[1, 0].set_xlabel('Recall')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].set_title('Curva Precision-Recall')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 5. Comparaci√≥n de M√©tricas Finales\n",
    "    metrics_names = ['Precision', 'Recall', 'F1-Score', 'AUC-PRC']\n",
    "    test_values = [\n",
    "        test_metrics['precision'],\n",
    "        test_metrics['recall'],\n",
    "        test_metrics['f1'],\n",
    "        test_metrics['prc_auc']\n",
    "    ]\n",
    "    val_values = [\n",
    "        val_metrics['precision'],\n",
    "        val_metrics['recall'],\n",
    "        val_metrics['f1'],\n",
    "        val_metrics['prc_auc']\n",
    "    ]\n",
    "\n",
    "    x = np.arange(len(metrics_names))\n",
    "    width = 0.35\n",
    "    axes[1, 1].bar(x - width/2, test_values, width, label='Test', color='skyblue')\n",
    "    axes[1, 1].bar(x + width/2, val_values, width, label='Validaci√≥n', color='lightcoral')\n",
    "    axes[1, 1].set_xlabel('M√©tricas')\n",
    "    axes[1, 1].set_ylabel('Valor')\n",
    "    axes[1, 1].set_title('Comparaci√≥n Test vs Validaci√≥n')\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(metrics_names, rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 6. Distribuci√≥n de Scores de Predicci√≥n\n",
    "    axes[1, 2].hist(scores, bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
    "    axes[1, 2].axvline(x=optimal_threshold, color='red', linestyle='--',\n",
    "                       label=f'Umbral={optimal_threshold:.3f}')\n",
    "    axes[1, 2].set_xlabel('Score de Predicci√≥n')\n",
    "    axes[1, 2].set_ylabel('Frecuencia')\n",
    "    axes[1, 2].set_title('Distribuci√≥n de Scores')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('gcn_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Obtener m√©tricas de validaci√≥n para comparaci√≥n\n",
    "model = GCNLinkPredictor(128, 256, 128)\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_val = model.encode(val_data.x, val_data.edge_index)\n",
    "    val_out = model.decode(z_val, val_data.edge_label_index)\n",
    "    val_pred_probs = torch.sigmoid(val_out).cpu().numpy()\n",
    "    val_true = val_data.edge_label.cpu().numpy()\n",
    "    val_metrics = calculate_metrics(val_pred_probs, val_true, optimal_threshold)\n",
    "\n",
    "# Generar visualizaci√≥n\n",
    "print(\"\\nüìä Generando visualizaci√≥n de m√©tricas...\")\n",
    "fig = plot_metrics_comparison(training_history, test_metrics, val_metrics)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 9: Reporte Final\n",
    "# -------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä REPORTE FINAL - GCN LINK PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìà M√âTRICAS DE EVALUACI√ìN:\")\n",
    "print(f\"  ‚Ä¢ AUC-ROC (Test): {test_auc:.4f}\")\n",
    "print(f\"  ‚Ä¢ AUC-PRC (Test): {test_metrics['prc_auc']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Precision (Test): {test_metrics['precision']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Recall (Test): {test_metrics['recall']:.4f}\")\n",
    "print(f\"  ‚Ä¢ F1-Score (Test): {test_metrics['f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ PREDICCIONES:\")\n",
    "print(f\"  ‚Ä¢ Umbral √≥ptimo: {optimal_threshold:.4f}\")\n",
    "print(f\"  ‚Ä¢ Predicciones generadas: {len(gcn_predictions)}\")\n",
    "print(f\"  ‚Ä¢ Aristas a√±adidas al grafo: {gcn_added_edges}\")\n",
    "print(f\"  ‚Ä¢ Score promedio de predicciones: {np.mean(scores):.4f}\")\n",
    "\n",
    "print(f\"\\nüìÅ GRAFO RESULTANTE:\")\n",
    "print(f\"  ‚Ä¢ Nodos: {G_gcn.number_of_nodes()}\")\n",
    "print(f\"  ‚Ä¢ Aristas: {G_gcn.number_of_edges()}\")\n",
    "print(f\"  ‚Ä¢ Densidad: {nx.density(G_gcn):.6f}\")\n",
    "\n",
    "# Guardar grafo enriquecido\n",
    "nx.write_gexf(G_gcn, \"knowledge_graph_gcn_enriched.gexf\")\n",
    "print(\"\\nüíæ Grafo GCN guardado como 'knowledge_graph_gcn_enriched.gexf'\")\n",
    "print(\"üíæ Gr√°fico de m√©tricas guardado como 'gcn_metrics_comparison.png'\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# PASO 10: Comparaci√≥n con PyKEEN (si lo tienes)\n",
    "# -------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚öñÔ∏è COMPARACI√ìN ENTRE M√âTODOS DE PREDICCI√ìN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Si tienes resultados de PyKEEN, puedes agregarlos aqu√≠:\n",
    "# pykeen_metrics = obtener_metricas_pykeen()  # Tu funci√≥n aqu√≠\n",
    "\n",
    "print(\"\\n‚úÖ GCN Link Prediction completado con todas las m√©tricas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "vVbTmRjI_6jm",
    "outputId": "49383617-892f-449a-89e1-a08c7f526ebe"
   },
   "outputs": [],
   "source": [
    "if 'G_gcn' in locals():\n",
    "    print(\"\\nüîç Validando grafo enriquecido con GCN...\")\n",
    "    # Usar tu validador existente\n",
    "    gcn_report = validator.validate_graph(G_gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2vv5yD4_6z3"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(gcn_report[\"valid_nodes\"])\n",
    "print(f\"Resultados validacion {gcn_report[\"edge_report\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kbgdzWNbHt0H",
    "outputId": "539878f9-813b-485c-fd6a-445c3efa4022"
   },
   "outputs": [],
   "source": [
    "!pip uninstall community\n",
    "!pip install python-louvain\n",
    "\n",
    "def analyze_class_imbalance(G_original):\n",
    "    \"\"\"Analiza el desequilibrio entre aristas existentes y posibles\"\"\"\n",
    "    n = G_original.number_of_nodes()\n",
    "    possible_edges = n * (n - 1) / 2 if n > 1 else 0\n",
    "    existing_edges = G_original.number_of_edges()\n",
    "    missing_edges = possible_edges - existing_edges\n",
    "\n",
    "    imbalance_ratio = missing_edges / existing_edges if existing_edges > 0 else float('inf')\n",
    "\n",
    "    print(f\"üìä An√°lisis de Desequilibrio de Clases:\")\n",
    "    print(f\"  ‚Ä¢ Nodos: {n}\")\n",
    "    print(f\"  ‚Ä¢ Aristas existentes (positivas): {existing_edges}\")\n",
    "    print(f\"  ‚Ä¢ Aristas posibles (total): {possible_edges:.0f}\")\n",
    "    print(f\"  ‚Ä¢ Aristas faltantes (negativas): {missing_edges:.0f}\")\n",
    "    print(f\"  ‚Ä¢ Ratio desequilibrio (neg:pos): {imbalance_ratio:.2f}:1\")\n",
    "    print(f\"  ‚Ä¢ % Aristas positivas: {(existing_edges/possible_edges)*100:.6f}%\")\n",
    "    print(f\"  ‚Ä¢ Densidad del grafo: {nx.density(G_original):.6f}\")\n",
    "\n",
    "    return imbalance_ratio\n",
    "\n",
    "def check_data_leakage(G_original, train_data, test_data, idx_to_node):\n",
    "    \"\"\"Verifica si hay leakage en la divisi√≥n train/test\"\"\"\n",
    "    print(\"\\nüîç Verificando Data Leakage...\")\n",
    "\n",
    "    # Verificar si hay nodos aislados\n",
    "    isolated_nodes = [n for n in G_original.nodes() if G_original.degree(n) == 0]\n",
    "    print(f\"  ‚Ä¢ Nodos aislados: {len(isolated_nodes)}\")\n",
    "\n",
    "    # Verificar aristas de test que podr√≠an ser predecibles por grado\n",
    "    test_positives = 0\n",
    "    easy_predictions = 0\n",
    "\n",
    "    for i in range(test_data.edge_label_index.shape[1]):\n",
    "        if test_data.edge_label[i].item() == 1:  # Arista positiva\n",
    "            u = test_data.edge_label_index[0, i].item()\n",
    "            v = test_data.edge_label_index[1, i].item()\n",
    "            test_positives += 1\n",
    "\n",
    "            # Verificar si ambos nodos tienen alto grado\n",
    "            if G_original.degree(idx_to_node[u]) > 5 and G_original.degree(idx_to_node[v]) > 5:\n",
    "                easy_predictions += 1\n",
    "\n",
    "    if test_positives > 0:\n",
    "        print(f\"  ‚Ä¢ Aristas test con ambos nodos alto grado: {(easy_predictions/test_positives)*100:.1f}%\")\n",
    "\n",
    "    # Verificar solapamiento train-test\n",
    "    train_edges_set = set()\n",
    "    for i in range(train_data.edge_index.shape[1]):\n",
    "        u = train_data.edge_index[0, i].item()\n",
    "        v = train_data.edge_index[1, i].item()\n",
    "        train_edges_set.add((min(u, v), max(u, v)))\n",
    "\n",
    "    test_train_overlap = 0\n",
    "    for i in range(test_data.edge_label_index.shape[1]):\n",
    "        if test_data.edge_label[i].item() == 1:\n",
    "            u = test_data.edge_label_index[0, i].item()\n",
    "            v = test_data.edge_label_index[1, i].item()\n",
    "            if (min(u, v), max(u, v)) in train_edges_set:\n",
    "                test_train_overlap += 1\n",
    "\n",
    "    if test_positives > 0:\n",
    "        print(f\"  ‚Ä¢ Solapamiento train-test: {(test_train_overlap/test_positives)*100:.1f}%\")\n",
    "\n",
    "    return len(isolated_nodes)\n",
    "\n",
    "def analyze_prediction_bias(predictions, test_pred_probs, test_true, G_original, idx_to_node):\n",
    "    \"\"\"Analiza el sesgo en las predicciones\"\"\"\n",
    "    print(\"\\nüéØ An√°lisis de Sesgo Predictivo:\")\n",
    "\n",
    "    # Distribuci√≥n de predicciones\n",
    "    threshold = 0.5\n",
    "    positive_preds = sum(test_pred_probs >= threshold)\n",
    "    negative_preds = sum(test_pred_probs < threshold)\n",
    "\n",
    "    print(f\"  ‚Ä¢ Predicciones positivas (‚â•{threshold}): {positive_preds}\")\n",
    "    print(f\"  ‚Ä¢ Predicciones negativas (<{threshold}): {negative_preds}\")\n",
    "    print(f\"  ‚Ä¢ Ratio positivo/negativo: {positive_preds/max(negative_preds, 1):.3f}\")\n",
    "\n",
    "    # Calcular m√©tricas espec√≠ficas\n",
    "    cm = confusion_matrix(test_true, (test_pred_probs >= threshold).astype(int))\n",
    "\n",
    "    print(f\"\\n  üìä Matriz de Confusi√≥n (threshold={threshold}):\")\n",
    "    print(f\"                    Predicho\")\n",
    "    print(f\"                  No      S√≠\")\n",
    "    print(f\"      Real No  [TN={cm[0,0]:4d}  FP={cm[0,1]:4d}]\")\n",
    "    print(f\"           S√≠  [FN={cm[1,0]:4d}  TP={cm[1,1]:4d}]\")\n",
    "\n",
    "    # M√©tricas detalladas\n",
    "    if cm[1,1] + cm[0,1] > 0:\n",
    "        precision = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    if cm[1,1] + cm[1,0] > 0:\n",
    "        recall = cm[1,1] / (cm[1,1] + cm[1,0])\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    print(f\"\\n  üìà M√©tricas por threshold {threshold}:\")\n",
    "    print(f\"      ‚Ä¢ Precision: {precision:.4f}\")\n",
    "    print(f\"      ‚Ä¢ Recall: {recall:.4f}\")\n",
    "    print(f\"      ‚Ä¢ F1-Score: {2*precision*recall/max(precision+recall, 1e-10):.4f}\")\n",
    "\n",
    "    # An√°lisis de scores de predicciones\n",
    "    if predictions:\n",
    "        scores = [p['score'] for p in predictions]\n",
    "        print(f\"\\n  üìä Estad√≠sticas de Scores de Predicci√≥n:\")\n",
    "        print(f\"      ‚Ä¢ M√≠nimo: {min(scores):.4f}\")\n",
    "        print(f\"      ‚Ä¢ M√°ximo: {max(scores):.4f}\")\n",
    "        print(f\"      ‚Ä¢ Mediana: {np.median(scores):.4f}\")\n",
    "        print(f\"      ‚Ä¢ Media: {np.mean(scores):.4f}\")\n",
    "        print(f\"      ‚Ä¢ Desviaci√≥n est√°ndar: {np.std(scores):.4f}\")\n",
    "\n",
    "        # Distribuci√≥n por percentiles\n",
    "        percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "        print(f\"      ‚Ä¢ Percentiles:\")\n",
    "        for p in percentiles:\n",
    "            print(f\"          P{p}: {np.percentile(scores, p):.4f}\")\n",
    "\n",
    "    # An√°lisis de correlaci√≥n con grado de nodos\n",
    "    print(f\"\\n  üîó Correlaci√≥n con Grado de Nodos:\")\n",
    "    degrees = []\n",
    "    pred_scores = []\n",
    "\n",
    "    for pred in predictions[:100]:  # Muestra de predicciones\n",
    "        node = pred['source']\n",
    "        if node in G_original:\n",
    "            degree = G_original.degree(node)\n",
    "            degrees.append(degree)\n",
    "            pred_scores.append(pred['score'])\n",
    "\n",
    "    if len(degrees) > 1:\n",
    "        correlation = np.corrcoef(degrees, pred_scores)[0, 1]\n",
    "        print(f\"      ‚Ä¢ Correlaci√≥n grado-score: {correlation:.4f}\")\n",
    "\n",
    "        if correlation > 0.7:\n",
    "            print(\"      ‚ö†Ô∏è  ALTO: El modelo puede estar prediciendo solo por grado alto\")\n",
    "        elif correlation > 0.4:\n",
    "            print(\"      ‚ÑπÔ∏è  MODERADO: El modelo considera el grado pero no exclusivamente\")\n",
    "        else:\n",
    "            print(\"      ‚úÖ BAJO: El modelo no depende fuertemente del grado\")\n",
    "\n",
    "def evaluate_negative_sampling(G_original, train_data, test_data, idx_to_node):\n",
    "    \"\"\"Eval√∫a si los negativos son demasiado f√°ciles de identificar\"\"\"\n",
    "    print(\"\\nüîÑ Evaluando Muestreo de Negativos...\")\n",
    "\n",
    "    # Calcular m√©tricas de los negativos generados\n",
    "    neg_degrees = []\n",
    "    pos_degrees = []\n",
    "\n",
    "    # Grados de nodos en aristas positivas (train)\n",
    "    for i in range(train_data.edge_index.shape[1]):\n",
    "        u = train_data.edge_index[0, i].item()\n",
    "        v = train_data.edge_index[1, i].item()\n",
    "        pos_degrees.append(G_original.degree(idx_to_node[u]))\n",
    "        pos_degrees.append(G_original.degree(idx_to_node[v]))\n",
    "\n",
    "    # Grados de nodos en aristas negativas (test)\n",
    "    for i in range(test_data.edge_label_index.shape[1]):\n",
    "        if test_data.edge_label[i].item() == 0:  # Arista negativa\n",
    "            u = test_data.edge_label_index[0, i].item()\n",
    "            v = test_data.edge_label_index[1, i].item()\n",
    "            neg_degrees.append(G_original.degree(idx_to_node[u]))\n",
    "            neg_degrees.append(G_original.degree(idx_to_node[v]))\n",
    "\n",
    "    if pos_degrees and neg_degrees:\n",
    "        print(f\"  ‚Ä¢ Grado promedio positivos: {np.mean(pos_degrees):.2f}\")\n",
    "        print(f\"  ‚Ä¢ Grado promedio negativos: {np.mean(neg_degrees):.2f}\")\n",
    "        print(f\"  ‚Ä¢ Diferencia: {np.mean(pos_degrees) - np.mean(neg_degrees):.2f}\")\n",
    "\n",
    "        # Test estad√≠stico simple\n",
    "        if abs(np.mean(pos_degrees) - np.mean(neg_degrees)) > 5:\n",
    "            print(f\"  ‚ö†Ô∏è  Los negativos tienen grado significativamente diferente\")\n",
    "\n",
    "    # Verificar si los negativos son triviales\n",
    "    trivial_negatives = 0\n",
    "    total_negatives = 0\n",
    "\n",
    "    for i in range(test_data.edge_label_index.shape[1]):\n",
    "        if test_data.edge_label[i].item() == 0:\n",
    "            u = test_data.edge_label_index[0, i].item()\n",
    "            v = test_data.edge_label_index[1, i].item()\n",
    "            total_negatives += 1\n",
    "\n",
    "            # Si los nodos est√°n en componentes conexas diferentes o muy lejanos\n",
    "            try:\n",
    "                if not nx.has_path(G_original, idx_to_node[u], idx_to_node[v]):\n",
    "                    trivial_negatives += 1\n",
    "            except:\n",
    "                trivial_negatives += 1\n",
    "\n",
    "    if total_negatives > 0:\n",
    "        trivial_percentage = trivial_negatives/total_negatives*100\n",
    "        print(f\"  ‚Ä¢ Negativos triviales (sin camino): {trivial_percentage:.1f}%\")\n",
    "\n",
    "        if trivial_percentage > 80:\n",
    "            print(f\"  ‚ö†Ô∏è  La mayor√≠a de negativos son triviales - AUC inflado\")\n",
    "\n",
    "    return trivial_percentage if total_negatives > 0 else 0\n",
    "\n",
    "def calculate_balanced_metrics(test_pred_probs, test_true):\n",
    "    \"\"\"Calcula m√©tricas en conjunto balanceado\"\"\"\n",
    "    print(\"\\nüìà Recalculo de M√©tricas con Balanceo:\")\n",
    "\n",
    "    # Identificar positivos y negativos\n",
    "    pos_indices = np.where(test_true == 1)[0]\n",
    "    neg_indices = np.where(test_true == 0)[0]\n",
    "\n",
    "    if len(pos_indices) == 0 or len(neg_indices) == 0:\n",
    "        print(\"  ‚ùå No hay suficientes muestras para balancear\")\n",
    "        return None, None\n",
    "\n",
    "    # Tomar muestra balanceada (1:1)\n",
    "    min_samples = min(len(pos_indices), len(neg_indices))\n",
    "\n",
    "    balanced_pos = np.random.choice(pos_indices, min_samples, replace=False)\n",
    "    balanced_neg = np.random.choice(neg_indices, min_samples, replace=False)\n",
    "\n",
    "    balanced_indices = np.concatenate([balanced_pos, balanced_neg])\n",
    "    balanced_true = test_true[balanced_indices]\n",
    "    balanced_pred = test_pred_probs[balanced_indices]\n",
    "\n",
    "    # Calcular m√©tricas balanceadas\n",
    "    balanced_auc = roc_auc_score(balanced_true, balanced_pred)\n",
    "\n",
    "    # Calcular m√©tricas originales para comparaci√≥n\n",
    "    original_auc = roc_auc_score(test_true, test_pred_probs)\n",
    "\n",
    "    print(f\"  ‚Ä¢ Muestras balanceadas: {2*min_samples} ({min_samples} positivas, {min_samples} negativas)\")\n",
    "    print(f\"  ‚Ä¢ AUC original (desbalanceado): {original_auc:.4f}\")\n",
    "    print(f\"  ‚Ä¢ AUC balanceado (1:1): {balanced_auc:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Diferencia: {original_auc - balanced_auc:+.4f}\")\n",
    "\n",
    "    if original_auc - balanced_auc > 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  Gran diferencia: AUC inflado por desbalance\")\n",
    "\n",
    "    # Calcular Precision-Recall AUC\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(test_true, test_pred_probs)\n",
    "    pr_auc = auc(recall_vals, precision_vals)\n",
    "\n",
    "    # PR AUC balanceado\n",
    "    precision_vals_bal, recall_vals_bal, _ = precision_recall_curve(balanced_true, balanced_pred)\n",
    "    pr_auc_bal = auc(recall_vals_bal, precision_vals_bal)\n",
    "\n",
    "    print(f\"  ‚Ä¢ PR-AUC original: {pr_auc:.4f}\")\n",
    "    print(f\"  ‚Ä¢ PR-AUC balanceado: {pr_auc_bal:.4f}\")\n",
    "\n",
    "    return balanced_auc, pr_auc\n",
    "\n",
    "def analyze_model_consistency(predictions, G_original, top_n=50):\n",
    "    \"\"\"Analiza la consistencia de las predicciones\"\"\"\n",
    "    print(\"\\nüîÑ Consistencia de Predicciones:\")\n",
    "\n",
    "    if not predictions:\n",
    "        print(\"  ‚ùå No hay predicciones para analizar\")\n",
    "        return\n",
    "\n",
    "    # Verificar variabilidad en scores\n",
    "    top_scores = [p['score'] for p in predictions[:top_n]]\n",
    "    unique_scores = len(set([round(s, 3) for s in top_scores]))\n",
    "\n",
    "    print(f\"  ‚Ä¢ Scores √∫nicos en top {top_n}: {unique_scores}\")\n",
    "\n",
    "    if unique_scores < top_n * 0.3:\n",
    "        print(f\"  ‚ö†Ô∏è  Poca variabilidad en predicciones\")\n",
    "\n",
    "    # Verificar patrones en predicciones\n",
    "    print(f\"\\n  üîé Patrones en Top {top_n} Predicciones:\")\n",
    "\n",
    "    # Contar predicciones intra-comunidad vs inter-comunidad\n",
    "    try:\n",
    "        import community as community_louvain\n",
    "        partition = community_louvain.best_partition(G_original)\n",
    "\n",
    "        intra_count = 0\n",
    "        inter_count = 0\n",
    "\n",
    "        for pred in predictions[:top_n]:\n",
    "            if pred['source'] in partition and pred['target'] in partition:\n",
    "                if partition[pred['source']] == partition[pred['target']]:\n",
    "                    intra_count += 1\n",
    "                else:\n",
    "                    inter_count += 1\n",
    "\n",
    "        total = intra_count + inter_count\n",
    "        if total > 0:\n",
    "            print(f\"      ‚Ä¢ Intra-comunidad: {intra_count} ({intra_count/total*100:.1f}%)\")\n",
    "            print(f\"      ‚Ä¢ Inter-comunidad: {inter_count} ({inter_count/total*100:.1f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚Ä¢ No se pudo analizar comunidades (instala python-louvain): {e}\")\n",
    "\n",
    "    # Verificar si predice a nodos populares\n",
    "    high_degree_nodes = [n for n in G_original.nodes() if G_original.degree(n) > 10]\n",
    "    predictions_to_high_degree = 0\n",
    "\n",
    "    for pred in predictions[:top_n]:\n",
    "        if pred['source'] in high_degree_nodes or pred['target'] in high_degree_nodes:\n",
    "            predictions_to_high_degree += 1\n",
    "\n",
    "    print(f\"      ‚Ä¢ Predicciones a nodos alto grado: {predictions_to_high_degree}/{top_n} ({predictions_to_high_degree/top_n*100:.1f}%)\")\n",
    "\n",
    "def comprehensive_diagnosis(G_original, G_gcn, predictions, test_pred_probs, test_true,\n",
    "                           train_data, test_data, idx_to_node):\n",
    "    \"\"\"Diagn√≥stico completo del aumento artificial de AUC\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"üî¨ DIAGN√ìSTICO COMPLETO - ¬øPOR QU√â AUMENTA EL AUC ARTIFICIALMENTE?\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. An√°lisis de desequilibrio\n",
    "    imbalance_ratio = analyze_class_imbalance(G_original)\n",
    "\n",
    "    # 2. Verificar data leakage\n",
    "    isolated_nodes = check_data_leakage(G_original, train_data, test_data, idx_to_node)\n",
    "\n",
    "    # 3. An√°lisis de sesgo predictivo\n",
    "    analyze_prediction_bias(predictions, test_pred_probs, test_true, G_original, idx_to_node)\n",
    "\n",
    "    # 4. Evaluar muestreo de negativos\n",
    "    trivial_percentage = evaluate_negative_sampling(G_original, train_data, test_data, idx_to_node)\n",
    "\n",
    "    # 5. Recalcular m√©tricas con balanceo\n",
    "    balanced_auc, pr_auc = calculate_balanced_metrics(test_pred_probs, test_true)\n",
    "\n",
    "    # 6. Analizar consistencia del modelo\n",
    "    analyze_model_consistency(predictions, G_original)\n",
    "\n",
    "    # 7. Resumen y conclusiones\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìã RESUMEN Y CONCLUSIONES\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    conclusions = []\n",
    "\n",
    "    if imbalance_ratio > 100:\n",
    "        conclusions.append(\"üî¥ DESBALANCE EXTREMO: AUC inflado por mayor√≠a de negativos\")\n",
    "    elif imbalance_ratio > 50:\n",
    "        conclusions.append(\"üü† DESBALANCE ALTO: Posible inflaci√≥n de AUC\")\n",
    "    else:\n",
    "        conclusions.append(\"üü¢ DESBALANCE MODERADO: AUC probablemente realista\")\n",
    "\n",
    "    if trivial_percentage > 80:\n",
    "        conclusions.append(\"üî¥ NEGATIVOS TRIVIALES: Negativos demasiado f√°ciles de identificar\")\n",
    "    elif trivial_percentage > 50:\n",
    "        conclusions.append(\"üü† NEGATIVOS F√ÅCILES: Algunos negativos son triviales\")\n",
    "    else:\n",
    "        conclusions.append(\"üü¢ NEGATIVOS ADECUADOS: Muestreo de negativos correcto\")\n",
    "\n",
    "    if isolated_nodes > G_original.number_of_nodes() * 0.1:\n",
    "        conclusions.append(\"üî¥ NODOS AISLADOS: Muchos nodos sin conexiones\")\n",
    "\n",
    "    # AUC balanceado vs original\n",
    "    original_auc = roc_auc_score(test_true, test_pred_probs)\n",
    "    if balanced_auc and (original_auc - balanced_auc) > 0.15:\n",
    "        conclusions.append(f\"üî¥ AUC INFLADO: Diferencia de {original_auc - balanced_auc:.3f} por desbalance\")\n",
    "    elif balanced_auc and (original_auc - balanced_auc) > 0.05:\n",
    "        conclusions.append(f\"üü† AUC MODERADAMENTE INFLADO: Diferencia de {original_auc - balanced_auc:.3f}\")\n",
    "\n",
    "    print(\"\\nüéØ Conclusiones principales:\")\n",
    "    for i, conclusion in enumerate(conclusions, 1):\n",
    "        print(f\"  {i}. {conclusion}\")\n",
    "\n",
    "    print(\"\\nüí° Recomendaciones:\")\n",
    "    print(\"  1. Usar AUC-PRC en lugar de AUC-ROC para datos desbalanceados\")\n",
    "    print(\"  2. Reportar Precision@K y Recall@K para evaluaciones pr√°cticas\")\n",
    "    print(\"  3. Implementar negative sampling m√°s desafiante\")\n",
    "    print(\"  4. Evaluar en diferentes thresholds\")\n",
    "    print(\"  5. Usar cross-validation por comunidades\")\n",
    "\n",
    "    # M√©tricas pr√°cticas\n",
    "    print(\"\\nüìä M√©tricas Pr√°cticas para Link Prediction:\")\n",
    "\n",
    "    def precision_at_k(model, test_data, k=100):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(test_data.x, test_data.edge_index)\n",
    "            scores = model.decode(z, test_data.edge_label_index).cpu().numpy()\n",
    "            true_labels = test_data.edge_label.cpu().numpy()\n",
    "            # Ordenar por score descendente\n",
    "            sorted_indices = np.argsort(scores)[::-1]\n",
    "            # Tomar top k\n",
    "            top_k_indices = sorted_indices[:k]\n",
    "            # Calcular precisi√≥n en top k\n",
    "            precision = precision_score(true_labels[top_k_indices],\n",
    "                                        (scores[top_k_indices] >= 0.5).astype(int))\n",
    "            return precision\n",
    "\n",
    "    precision_results = {}\n",
    "\n",
    "    # Calcular Precision@K\n",
    "    for k in [10, 20, 50, 100]:\n",
    "        prec = precision_at_k(model, test_data, k=k)\n",
    "        print(f\"Precision@{k}: {prec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'trivial_negatives_percentage': trivial_percentage,\n",
    "        'original_auc': original_auc,\n",
    "        'balanced_auc': balanced_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 3: FUNCI√ìN PARA VISUALIZAR RESULTADOS\n",
    "# ============================================================================\n",
    "def visualize_diagnosis_results(G_original, predictions, test_pred_probs, test_true,\n",
    "                               diagnosis_results, top_k=50):\n",
    "    \"\"\"Genera visualizaciones para el diagn√≥stico\"\"\"\n",
    "    print(\"\\nüìä Generando visualizaciones...\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # 1. Distribuci√≥n de grados del grafo\n",
    "    degrees = [d for n, d in G_original.degree()]\n",
    "    axes[0, 0].hist(degrees, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_xlabel('Grado del nodo')\n",
    "    axes[0, 0].set_ylabel('Frecuencia')\n",
    "    axes[0, 0].set_title('Distribuci√≥n de Grados del Grafo')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Distribuci√≥n de scores de predicci√≥n\n",
    "    if predictions:\n",
    "        scores = [p['score'] for p in predictions[:top_k*2]]\n",
    "        axes[0, 1].hist(scores, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "        axes[0, 1].axvline(x=0.5, color='red', linestyle='--', label='Umbral 0.5')\n",
    "        axes[0, 1].set_xlabel('Score de predicci√≥n')\n",
    "        axes[0, 1].set_ylabel('Frecuencia')\n",
    "        axes[0, 1].set_title('Distribuci√≥n de Scores de Predicci√≥n')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Curva Precision-Recall\n",
    "    precision_vals, recall_vals, thresholds = precision_recall_curve(test_true, test_pred_probs)\n",
    "    axes[0, 2].plot(recall_vals, precision_vals, 'b-', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Recall')\n",
    "    axes[0, 2].set_ylabel('Precision')\n",
    "    axes[0, 2].set_title(f'Curva Precision-Recall (AUC={diagnosis_results.get(\"pr_auc\", 0):.3f})')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Comparaci√≥n de m√©tricas\n",
    "    metrics_to_compare = ['AUC-ROC', 'AUC-PRC']\n",
    "    values = [\n",
    "        diagnosis_results.get('original_auc', 0),\n",
    "        diagnosis_results.get('pr_auc', 0)\n",
    "    ]\n",
    "\n",
    "    if diagnosis_results.get('balanced_auc'):\n",
    "        metrics_to_compare.append('AUC-ROC Balanceado')\n",
    "        values.append(diagnosis_results['balanced_auc'])\n",
    "\n",
    "    x_pos = np.arange(len(metrics_to_compare))\n",
    "    bars = axes[1, 0].bar(x_pos, values, color=['skyblue', 'lightcoral', 'lightgreen'][:len(values)])\n",
    "    axes[1, 0].set_ylabel('Valor')\n",
    "    axes[1, 0].set_title('Comparaci√≥n de M√©tricas')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(metrics_to_compare, rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # A√±adir valores encima de las barras\n",
    "    for bar, val in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{val:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    # 5. Precision@K\n",
    "    if 'precision_at_k' in diagnosis_results:\n",
    "        k_values = list(diagnosis_results['precision_at_k'].keys())\n",
    "        prec_values = list(diagnosis_results['precision_at_k'].values())\n",
    "\n",
    "        axes[1, 1].plot(k_values, prec_values, 'go-', linewidth=2, markersize=8)\n",
    "        axes[1, 1].set_xlabel('K')\n",
    "        axes[1, 1].set_ylabel('Precision')\n",
    "        axes[1, 1].set_title('Precision@K')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # 6. Matriz de confusi√≥n (threshold=0.5)\n",
    "    cm = confusion_matrix(test_true, (test_pred_probs >= 0.5).astype(int))\n",
    "\n",
    "    im = axes[1, 2].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    axes[1, 2].set_title('Matriz de Confusi√≥n (threshold=0.5)')\n",
    "\n",
    "    # A√±adir texto en las celdas\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            axes[1, 2].text(j, i, format(cm[i, j], 'd'),\n",
    "                          ha=\"center\", va=\"center\",\n",
    "                          color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    axes[1, 2].set_ylabel('Etiqueta Real')\n",
    "    axes[1, 2].set_xlabel('Etiqueta Predicha')\n",
    "    axes[1, 2].set_xticks([0, 1])\n",
    "    axes[1, 2].set_yticks([0, 1])\n",
    "    axes[1, 2].set_xticklabels(['No', 'S√≠'])\n",
    "    axes[1, 2].set_yticklabels(['No', 'S√≠'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('diagnosis_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# ============================================================================\n",
    "# PASO 4: EJECUTAR DIAGN√ìSTICO COMPLETO\n",
    "# ============================================================================\n",
    "print(\"üöÄ Iniciando diagn√≥stico completo...\")\n",
    "\n",
    "# Verificar que tenemos todos los datos necesarios\n",
    "required_vars = ['G_original', 'G_gcn', 'gcn_predictions', 'test_pred_probs',\n",
    "                 'test_true', 'train_data', 'test_data', 'idx_to_node']\n",
    "\n",
    "missing_vars = []\n",
    "for var in required_vars:\n",
    "    if var not in locals():\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"‚ö†Ô∏è Variables faltantes: {missing_vars}\")\n",
    "    print(\"Por favor, aseg√∫rate de haber ejecutado el entrenamiento del modelo GCN primero.\")\n",
    "else:\n",
    "    # Ejecutar diagn√≥stico\n",
    "    diagnosis_results = comprehensive_diagnosis(\n",
    "        G_original=G_original,\n",
    "        G_gcn=G_gcn,\n",
    "        predictions=gcn_predictions,\n",
    "        test_pred_probs=test_pred_probs,\n",
    "        test_true=test_true,\n",
    "        train_data=train_data,\n",
    "        test_data=test_data,\n",
    "        idx_to_node=idx_to_node\n",
    "    )\n",
    "\n",
    "    # Generar visualizaciones\n",
    "    fig = visualize_diagnosis_results(\n",
    "        G_original=G_original,\n",
    "        predictions=gcn_predictions,\n",
    "        test_pred_probs=test_pred_probs,\n",
    "        test_true=test_true,\n",
    "        diagnosis_results=diagnosis_results,\n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ DIAGN√ìSTICO COMPLETADO\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nLos resultados se han guardado en 'diagnosis_results.png'\")\n",
    "    print(\"Revisa las conclusiones y recomendaciones arriba.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edK2sv3I4kbl"
   },
   "source": [
    "### Clase ExtendedGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbeAYuqSLhMq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "from pykeen.predict import predict_target\n",
    "\n",
    "class ExtendedGraph(Graph):\n",
    "    def __init__(self, graph):\n",
    "        super().__init__()\n",
    "        self.graph = graph.graph.copy()\n",
    "        self.predicted_edges = []\n",
    "\n",
    "    def predict_edges(self, relacion_busqueda=\"relacionado_con\", n_predicciones=10, epochs=100):\n",
    "        \"\"\"\n",
    "        Entrena un modelo de Knowledge Graph Embedding y a√±ade nuevas aristas al grafo original.\n",
    "        Modifica el grafo actual y almacena las aristas predichas en self.predicted_edges.\n",
    "\n",
    "        Args:\n",
    "            relacion_busqueda (str): Tipo de relaci√≥n a predecir. Por defecto \"relacionado_con\".\n",
    "            n_predicciones (int): N√∫mero de predicciones top a considerar por nodo. Por defecto 10.\n",
    "            epochs (int): N√∫mero de √©pocas para entrenar el modelo. Por defecto 100.\n",
    "        \"\"\"\n",
    "        triples_list = []\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            rel = data.get('relation', 'relacionado_con')\n",
    "            triples_list.append([str(u), str(rel), str(v)])\n",
    "\n",
    "        df_triples = pd.DataFrame(triples_list, columns=['head', 'relation', 'tail'])\n",
    "\n",
    "        tf = TriplesFactory.from_labeled_triples(triples=df_triples.values)\n",
    "        training_factory, testing_factory = tf.split([0.8, 0.2], random_state=42)\n",
    "\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"Usando dispositivo: {device}\")\n",
    "        print(f\"Entrenando modelo para {len(self.graph.nodes)} nodos...\")\n",
    "\n",
    "        result = pipeline(\n",
    "            training=training_factory,\n",
    "            testing=testing_factory,\n",
    "            model='RotatE',\n",
    "            epochs=epochs,\n",
    "            device=device,\n",
    "            random_seed=42\n",
    "        )\n",
    "\n",
    "        for u, v in self.graph.edges():\n",
    "            self.graph[u][v]['origin'] = 'real'\n",
    "\n",
    "        print(\"Generando predicciones de nuevas conexiones...\")\n",
    "\n",
    "        self.predicted_edges = []\n",
    "\n",
    "        for nodo in self.graph.nodes():\n",
    "            try:\n",
    "                predicciones = predict_target(\n",
    "                    model=result.model,\n",
    "                    head=str(nodo),\n",
    "                    relation=relacion_busqueda,\n",
    "                    triples_factory=tf\n",
    "                ).df\n",
    "\n",
    "                top_preds = predicciones.sort_values(by='score', ascending=False).head(n_predicciones)\n",
    "\n",
    "                for _, row in top_preds.iterrows():\n",
    "                    target = row['tail_label']\n",
    "                    score = row['score']\n",
    "\n",
    "                    if not self.graph.has_edge(nodo, target) and nodo != target:\n",
    "                        self.graph.add_edge(nodo, target,\n",
    "                                           relation=relacion_busqueda,\n",
    "                                           origin='predicha',\n",
    "                                           weight=score)\n",
    "                        self.predicted_edges.append({\n",
    "                            'source': nodo,\n",
    "                            'target': target,\n",
    "                            'relation': relacion_busqueda,\n",
    "                            'score': score\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"¬°Proceso completado! Aristas totales: {self.graph.number_of_edges()}\")\n",
    "        print(f\"Aristas predichas a√±adidas: {len(self.predicted_edges)}\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    def print_img(self, remove_outliers=True, img_name=\"extended_graph.png\"):\n",
    "        \"\"\"\n",
    "        Imprime el grafo enriquecido, diferenciando entre aristas reales y predichas.\n",
    "\n",
    "        Args:\n",
    "            remove_outliers (bool): Si es True, elimina nodos con grado 1 para mejor visualizaci√≥n.\n",
    "            img_name (str): Nombre del archivo de imagen donde se guardar√° el grafo.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        G_viz = self.graph.copy()\n",
    "\n",
    "        if remove_outliers:\n",
    "            low_degree_nodes = [node for node, degree in dict(G_viz.degree()).items() if degree <= 1]\n",
    "            G_viz.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "        pos = nx.spring_layout(G_viz, seed=42)\n",
    "\n",
    "        plt.figure(figsize=(12, 12))\n",
    "\n",
    "        nx.draw_networkx_nodes(G_viz, pos, node_size=300, node_color='lightblue')\n",
    "\n",
    "        real_edges = [(u, v) for u, v, d in G_viz.edges(data=True) if d.get('origin') == 'real']\n",
    "        nx.draw_networkx_edges(G_viz, pos, edgelist=real_edges, edge_color='green', label='Real', width=2)\n",
    "\n",
    "        predicted_edges = [(u, v) for u, v, d in G_viz.edges(data=True) if d.get('origin') == 'predicha']\n",
    "        nx.draw_networkx_edges(G_viz, pos, edgelist=predicted_edges, edge_color='red', style='dashed', label='Predicha', width=2)\n",
    "\n",
    "        nx.draw_networkx_labels(G_viz, pos, font_size=10)\n",
    "\n",
    "        plt.title(\"Grafo Enriquecido con Predicciones de Conexiones\", fontsize=15)\n",
    "        plt.legend(scatterpoints=1)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(img_name)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def print_predictions(self):\n",
    "        \"\"\"\n",
    "        Imprime las aristas que fueron predichas por el modelo.\n",
    "        \"\"\"\n",
    "        if not self.predicted_edges:\n",
    "            print(\"No hay aristas predichas almacenadas.\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nAristas Predichas ({len(self.predicted_edges)} total):\")\n",
    "        print(\"-\" * 80)\n",
    "        for edge in self.predicted_edges:\n",
    "            print(f\"{edge['source']} --({edge['relation']}, score: {edge['score']:.4f})--> {edge['target']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    def get_predicted_edges_dataframe(self):\n",
    "        \"\"\"\n",
    "        Retorna un DataFrame de pandas con las aristas predichas.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame con columnas 'source', 'target', 'relation', 'score'\n",
    "        \"\"\"\n",
    "        if not self.predicted_edges:\n",
    "            return pd.DataFrame(columns=['source', 'target', 'relation', 'score'])\n",
    "        return pd.DataFrame(self.predicted_edges)\n",
    "\n",
    "    def get_extended_graph(self):\n",
    "        \"\"\"\n",
    "        Retorna un nuevo objeto Graph que contiene el grafo extendido con todas las aristas\n",
    "        (originales + predichas).\n",
    "\n",
    "        Returns:\n",
    "            Graph: Nuevo objeto Graph con todas las aristas incluidas.\n",
    "        \"\"\"\n",
    "        extended_graph_obj = Graph()\n",
    "        extended_graph_obj.graph = self.graph.copy()\n",
    "        return extended_graph_obj\n",
    "\n",
    "    def export_predicted_edges_img(self, remove_outliers=True, img_name=\"predicted_edges_only.png\"):\n",
    "        \"\"\"\n",
    "        Exporta una imagen mostrando √∫nicamente las aristas predichas por el modelo.\n",
    "\n",
    "        Args:\n",
    "            remove_outliers (bool): Si es True, elimina nodos con grado 1 para mejor visualizaci√≥n.\n",
    "            img_name (str): Nombre del archivo de imagen donde se guardar√° el grafo.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        G_predicted = nx.DiGraph()\n",
    "\n",
    "        for u, v, data in self.graph.edges(data=True):\n",
    "            if data.get('origin') == 'predicha':\n",
    "                G_predicted.add_edge(u, v, **data)\n",
    "\n",
    "        if G_predicted.number_of_edges() == 0:\n",
    "            print(\"No hay aristas predichas para visualizar.\")\n",
    "            return\n",
    "\n",
    "        if remove_outliers:\n",
    "            low_degree_nodes = [node for node, degree in dict(G_predicted.degree()).items() if degree <= 1]\n",
    "            G_predicted.remove_nodes_from(low_degree_nodes)\n",
    "\n",
    "        if G_predicted.number_of_nodes() == 0:\n",
    "            print(\"No hay nodos para visualizar despu√©s de remover outliers.\")\n",
    "            return\n",
    "\n",
    "        pos = nx.spring_layout(G_predicted, seed=42, k=2, iterations=50)\n",
    "\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        nx.draw_networkx_nodes(G_predicted, pos, node_size=500, node_color='lightcoral', alpha=0.9)\n",
    "\n",
    "        edges = list(G_predicted.edges(data=True))\n",
    "        weights = [d.get('weight', 0.5) for _, _, d in edges]\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G_predicted,\n",
    "            pos,\n",
    "            edge_color=weights,\n",
    "            edge_cmap=plt.cm.Reds,\n",
    "            width=2,\n",
    "            arrows=True,\n",
    "            arrowsize=20,\n",
    "            arrowstyle='->',\n",
    "            connectionstyle='arc3,rad=0.1'\n",
    "        )\n",
    "\n",
    "        nx.draw_networkx_labels(G_predicted, pos, font_size=10, font_weight='bold')\n",
    "\n",
    "        edge_labels = {(u, v): f\"{d.get('weight', 0):.3f}\" for u, v, d in G_predicted.edges(data=True)}\n",
    "        nx.draw_networkx_edge_labels(G_predicted, pos, edge_labels, font_size=8)\n",
    "\n",
    "        plt.title(f\"Aristas Predichas por el Modelo\\n({G_predicted.number_of_edges()} conexiones predichas)\",\n",
    "                  fontsize=16, fontweight='bold')\n",
    "\n",
    "        sm = plt.cm.ScalarMappable(cmap=plt.cm.Reds, norm=plt.Normalize(vmin=min(weights), vmax=max(weights)))\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, label='Score de Confianza', ax=plt.gca(), shrink=0.8)\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_name, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Imagen exportada exitosamente: {img_name}\")\n",
    "        print(f\"Nodos en el grafo: {G_predicted.number_of_nodes()}\")\n",
    "        print(f\"Aristas predichas: {G_predicted.number_of_edges()}\")\n",
    "\n",
    "    def export_predicted_edges_txt(self, txt_name=\"predicted_edges.txt\", format=\"detailed\"):\n",
    "        \"\"\"\n",
    "        Exporta las aristas predichas a un archivo de texto.\n",
    "\n",
    "        Args:\n",
    "            txt_name (str): Nombre del archivo de texto donde se guardar√°n las aristas.\n",
    "            format (str): Formato de exportaci√≥n. Opciones:\n",
    "                - \"detailed\": Formato detallado con toda la informaci√≥n\n",
    "                - \"simple\": Formato simple (source -> target)\n",
    "                - \"csv\": Formato CSV separado por comas\n",
    "                - \"tsv\": Formato TSV separado por tabulaciones\n",
    "        \"\"\"\n",
    "        if not self.predicted_edges:\n",
    "            print(\"No hay aristas predichas para exportar.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(txt_name, 'w', encoding='utf-8') as f:\n",
    "                if format == \"detailed\":\n",
    "                    f.write(\"=\" * 80 + \"\\n\")\n",
    "                    f.write(\"ARISTAS PREDICHAS POR EL MODELO\\n\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\")\n",
    "                    f.write(f\"Total de aristas predichas: {len(self.predicted_edges)}\\n\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                    for i, edge in enumerate(self.predicted_edges, 1):\n",
    "                        f.write(f\"{i}. {edge['source']} --({edge['relation']}, score: {edge['score']:.4f})--> {edge['target']}\\n\")\n",
    "\n",
    "                elif format == \"simple\":\n",
    "                    for edge in self.predicted_edges:\n",
    "                        f.write(f\"{edge['source']} -> {edge['target']}\\n\")\n",
    "\n",
    "                elif format == \"csv\":\n",
    "                    f.write(\"source,target,relation,score\\n\")\n",
    "                    for edge in self.predicted_edges:\n",
    "                        f.write(f\"{edge['source']},{edge['target']},{edge['relation']},{edge['score']:.4f}\\n\")\n",
    "\n",
    "                elif format == \"tsv\":\n",
    "                    f.write(\"source\\ttarget\\trelation\\tscore\\n\")\n",
    "                    for edge in self.predicted_edges:\n",
    "                        f.write(f\"{edge['source']}\\t{edge['target']}\\t{edge['relation']}\\t{edge['score']:.4f}\\n\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Formato '{format}' no reconocido. Usando formato 'detailed'.\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\")\n",
    "                    f.write(\"ARISTAS PREDICHAS POR EL MODELO\\n\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\")\n",
    "                    f.write(f\"Total de aristas predichas: {len(self.predicted_edges)}\\n\")\n",
    "                    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "                    for i, edge in enumerate(self.predicted_edges, 1):\n",
    "                        f.write(f\"{i}. {edge['source']} --({edge['relation']}, score: {edge['score']:.4f})--> {edge['target']}\\n\")\n",
    "\n",
    "            print(f\"Aristas predichas exportadas exitosamente a: {txt_name}\")\n",
    "            print(f\"Formato: {format}\")\n",
    "            print(f\"Total de aristas: {len(self.predicted_edges)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error al exportar las aristas predichas: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1bkqfVLqrnW"
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "g.graph = G.copy()\n",
    "print(\"Esta es la evaluacion de la topologia del grafo\")\n",
    "print(g.analyze_topology())\n",
    "\n",
    "top_nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(\"\\nüëë Top 10 Nodos (Hubs) del Grafo:\")\n",
    "for node, degree in top_nodes:\n",
    "    print(f\"   ‚Ä¢ {node}: {degree} conexiones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7sljBUP4py8"
   },
   "source": [
    "### Predecir aristas faltantes de $G$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sa1QB4KLwSF"
   },
   "outputs": [],
   "source": [
    "graph = Graph()\n",
    "graph.graph = G.copy()\n",
    "extended_graph = ExtendedGraph(graph)\n",
    "result = extended_graph.predict_edges()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8TDXCcdonJr"
   },
   "outputs": [],
   "source": [
    "metrics = result.metric_results.to_df()\n",
    "\n",
    "print(\"\\nüìä M√©tricas de Calidad del Modelo KGE (Vista Resumida):\")\n",
    "print(\"--------------------------------------------------\")\n",
    "\n",
    "# Filtrar m√©tricas clave y formato 'realistic' para 'both' sides\n",
    "key_metrics = [\n",
    "    'mean_reciprocal_rank', # Intentar con este nombre si existe\n",
    "    'inverse_harmonic_mean_rank', # Nombre com√∫n para MRR en PyKEEN\n",
    "    'hits_at_1',\n",
    "    'hits_at_3',\n",
    "    'hits_at_10'\n",
    "]\n",
    "\n",
    "filtered_metrics = metrics[\n",
    "    (metrics['Rank_type'] == 'realistic') &\n",
    "    (metrics['Side'] == 'both') &\n",
    "    (metrics['Metric'].isin(key_metrics))\n",
    "]\n",
    "\n",
    "# Pivotar para una mejor visualizaci√≥n\n",
    "pivoted_metrics = filtered_metrics.pivot_table(index=['Metric'], columns='Side', values='Value')\n",
    "\n",
    "# Ajustar el nombre de la columna para 'both' si es necesario\n",
    "pivoted_metrics.columns = ['Value (both sides)']\n",
    "\n",
    "display(pivoted_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3y922kMRUKq"
   },
   "outputs": [],
   "source": [
    "extended_graph.export_predicted_edges_img()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEx7jiYe41rb"
   },
   "source": [
    "## Comparar G con Grafo extendido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEsS-incZcRV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import networkx as nx\n",
    "from difflib import get_close_matches\n",
    "from owlready2 import get_ontology, World\n",
    "\n",
    "# 2. Verificaci√≥n de variables de entrada\n",
    "if 'G' not in locals() or 'extended_graph' not in locals():\n",
    "    print(\"‚ùå Error: Debes tener definidos 'G' (original) y 'extended_graph' (con atributo .graph).\")\n",
    "else:\n",
    "    G_original = G\n",
    "    G_extended = extended_graph.graph\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPARACI√ìN: Grafo Original vs Grafo Extendido\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_results = []\n",
    "\n",
    "    print(f\"{'Ontolog√≠a':<25} | {'Aristas Originales (V/I)':<25} | {'Aristas Extendidas (V/I)':<25} | {'Mejora'}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, url in ontologies_map.items():\n",
    "        try:\n",
    "            # Validar ambos grafos\n",
    "            rep_orig = validator.validate_graph(G_original)\n",
    "            rep_ext = validator.validate_graph(G_extended)\n",
    "\n",
    "            # Acceder a los datos del edge_report\n",
    "            edge_report_orig = rep_orig['edge_report']\n",
    "            edge_report_ext = rep_ext['edge_report']\n",
    "\n",
    "            # C√°lculo de mejora en aristas v√°lidas\n",
    "            improvement = edge_report_ext['valid_rels'] - edge_report_orig['valid_rels']\n",
    "            improvement_pct = (improvement / max(1, edge_report_orig['valid_rels'])) * 100 if edge_report_orig['valid_rels'] > 0 else (100 if improvement > 0 else 0)\n",
    "            symbol = \"‚úÖ\" if improvement > 0 else (\"‚ûñ\" if improvement == 0 else \"‚ùå\")\n",
    "\n",
    "            # Formateo de tabla\n",
    "            orig_str = f\"{edge_report_orig['valid_rels']}/{edge_report_orig['weak_rels']}\"\n",
    "            ext_str = f\"{edge_report_ext['valid_rels']}/{edge_report_ext['weak_rels']}\"\n",
    "            imp_str = f\"{symbol} {improvement:+d} ({improvement_pct:+.1f}%)\"\n",
    "\n",
    "            print(f\"{name:<25} | {orig_str:<25} | {ext_str:<25} | {imp_str:<15}\")\n",
    "\n",
    "            comparison_results.append({\n",
    "                \"ontology\": name,\n",
    "                \"original_valid_edges\": edge_report_orig['valid_rels'],\n",
    "                \"original_invalid_edges\": edge_report_orig['weak_rels'],\n",
    "                \"extended_valid_edges\": edge_report_ext['valid_rels'],\n",
    "                \"extended_invalid_edges\": edge_report_ext['weak_rels'],\n",
    "                \"improvement_absolute\": improvement,\n",
    "                \"improvement_percentage\": improvement_pct\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{name:<25} | ‚ùå ERROR: {str(e)}\")\n",
    "\n",
    "    print(\"-\" * 95)\n",
    "    # An√°lisis detallado de las aristas predichas\n",
    "    if comparison_results:\n",
    "        # Encontrar la ontolog√≠a con la mayor mejora en aristas v√°lidas\n",
    "        best_improvement = -float('inf')\n",
    "        best_ontology_name = None\n",
    "        best_ontology_report = None\n",
    "\n",
    "        for res in comparison_results:\n",
    "            if res['improvement_absolute'] > best_improvement:\n",
    "                best_improvement = res['improvement_absolute']\n",
    "                best_ontology_name = res['ontology']\n",
    "                # Need to re-validate the extended graph with this specific validator for details\n",
    "                # This is a bit inefficient but ensures we use the correct ontology for details\n",
    "                current_validator = GraphValidator(loaded_ontologies={name: loaded_ontologies.get(name)})\n",
    "                best_ontology_report = current_validator.validate_graph(G_extended)\n",
    "\n",
    "        if best_ontology_name:\n",
    "            print(f\"\\nüèÜ Mayor mejora en aristas v√°lidas: {best_ontology_name}\")\n",
    "\n",
    "        # Obtener aristas nuevas (solo las que son predichas y a√±adidas al grafo extendido)\n",
    "        new_edges_in_extended = []\n",
    "        for u, v, data in G_extended.edges(data=True):\n",
    "            if data.get('origin') == 'predicha': # Asumiendo que 'origin' es una propiedad que marca las aristas predichas\n",
    "                new_edges_in_extended.append((u, v, data.get('relation', 'unknown')))\n",
    "\n",
    "        if new_edges_in_extended:\n",
    "            strong_new_edges_count = 0\n",
    "            if best_ontology_report: # Use the validator from the best ontology for detailed check\n",
    "                # Extract the validator instance from the context that produced best_ontology_report\n",
    "                # This is a bit indirect, but we need an actual validator object\n",
    "                # Re-initializing is simpler for this example.\n",
    "                detailed_validator = GraphValidator(ontology_urls=[ontologies_map[best_ontology_name]])\n",
    "\n",
    "                for u, v, rel in new_edges_in_extended:\n",
    "                    edge_validation_result = detailed_validator.validate_edge(u, v, rel)\n",
    "                    if edge_validation_result['fully_valid']:\n",
    "                        strong_new_edges_count += 1\n",
    "\n",
    "            print(f\"\\nüîç An√°lisis de {len(new_edges_in_extended)} aristas predichas a√±adidas:\")\n",
    "            print(f\"   ‚Ä¢ Validadas ontol√≥gicamente (por {best_ontology_name}): {strong_new_edges_count} ({(strong_new_edges_count/len(new_edges_in_extended))*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nüîç No se encontraron aristas predichas a√±adidas al grafo extendido para analizar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzUlGvZKaaM1"
   },
   "source": [
    "# Etiquetar aristas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfKb3XHEGOgj"
   },
   "outputs": [],
   "source": [
    "# Hazte tu api key de mistral en https://admin.mistral.ai/organization/api-keys para etiquetar las aristas\n",
    "api_key = \"2e7t2KjNXDm6V3eB1Zlr9U0W4zJkUNxU\"\n",
    "\n",
    "if api_key == \"api_key\":\n",
    "    raise Exception(\"Genera tu api_key de mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywkXj4AjfKfV"
   },
   "outputs": [],
   "source": [
    "labeled_graph = Graph()\n",
    "labeled_graph.graph = G\n",
    "labeled_graph.label_edges(api_key, use_cache=False)\n",
    "labeled_graph = ExtendedGraph(labeled_graph)\n",
    "labeled_graph.predict_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbapTTxHFdwe"
   },
   "source": [
    "### Evaluar grafo etiquetado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hd4nowT_fnct"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import networkx as nx\n",
    "from difflib import get_close_matches\n",
    "from owlready2 import get_ontology, World\n",
    "\n",
    "# 2. Verificaci√≥n de variables de entrada\n",
    "if 'G' not in locals() or 'extended_graph' not in locals():\n",
    "    print(\"‚ùå Error: Debes tener definidos 'G' (original) y 'extended_graph' (con atributo .graph).\")\n",
    "else:\n",
    "    G_original = G\n",
    "    G_extended = labeled_graph.graph\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPARACI√ìN: Grafo Original vs Grafo Extendido\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    comparison_results = []\n",
    "\n",
    "    print(f\"{'Ontolog√≠a':<25} | {'Aristas Originales (V/I)':<25} | {'Aristas Extendidas (V/I)':<25} | {'Mejora'}\")\n",
    "    print(\"-\" * 95)\n",
    "\n",
    "    for name, url in ontologies_map.items():\n",
    "        try:\n",
    "            # Validar ambos grafos\n",
    "            rep_orig = validator.validate_graph(G_original)\n",
    "            rep_ext = validator.validate_graph(G_extended)\n",
    "\n",
    "            # Acceder a los datos del edge_report\n",
    "            edge_report_orig = rep_orig['edge_report']\n",
    "            edge_report_ext = rep_ext['edge_report']\n",
    "\n",
    "            # C√°lculo de mejora en aristas v√°lidas\n",
    "            improvement = edge_report_ext['valid_rels'] - edge_report_orig['valid_rels']\n",
    "            improvement_pct = (improvement / max(1, edge_report_orig['valid_rels'])) * 100 if edge_report_orig['valid_rels'] > 0 else (100 if improvement > 0 else 0)\n",
    "            symbol = \"‚úÖ\" if improvement > 0 else (\"‚ûñ\" if improvement == 0 else \"‚ùå\")\n",
    "\n",
    "            # Formateo de tabla\n",
    "            orig_str = f\"{edge_report_orig['valid_rels']}/{edge_report_orig['weak_rels']}\"\n",
    "            ext_str = f\"{edge_report_ext['valid_rels']}/{edge_report_ext['weak_rels']}\"\n",
    "            imp_str = f\"{symbol} {improvement:+d} ({improvement_pct:+.1f}%)\"\n",
    "\n",
    "            print(f\"{name:<25} | {orig_str:<25} | {ext_str:<25} | {imp_str:<15}\")\n",
    "\n",
    "            comparison_results.append({\n",
    "                \"ontology\": name,\n",
    "                \"original_valid_edges\": edge_report_orig['valid_rels'],\n",
    "                \"original_invalid_edges\": edge_report_orig['weak_rels'],\n",
    "                \"extended_valid_edges\": edge_report_ext['valid_rels'],\n",
    "                \"extended_invalid_edges\": edge_report_ext['weak_rels'],\n",
    "                \"improvement_absolute\": improvement,\n",
    "                \"improvement_percentage\": improvement_pct\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{name:<25} | ‚ùå ERROR: {str(e)}\")\n",
    "\n",
    "    print(\"-\" * 95)\n",
    "    # An√°lisis detallado de las aristas predichas\n",
    "    if comparison_results:\n",
    "        # Encontrar la ontolog√≠a con la mayor mejora en aristas v√°lidas\n",
    "        best_improvement = -float('inf')\n",
    "        best_ontology_name = None\n",
    "        best_ontology_report = None\n",
    "\n",
    "        for res in comparison_results:\n",
    "            if res['improvement_absolute'] > best_improvement:\n",
    "                best_improvement = res['improvement_absolute']\n",
    "                best_ontology_name = res['ontology']\n",
    "                # Need to re-validate the extended graph with this specific validator for details\n",
    "                # This is a bit inefficient but ensures we use the correct ontology for details\n",
    "                current_validator = GraphValidator(ontology_urls=[ontologies_map[res['ontology']]])\n",
    "                best_ontology_report = current_validator.validate_graph(G_extended)\n",
    "\n",
    "        if best_ontology_name:\n",
    "            print(f\"\\nüèÜ Mayor mejora en aristas v√°lidas: {best_ontology_name}\")\n",
    "\n",
    "        # Obtener aristas nuevas (solo las que son predichas y a√±adidas al grafo extendido)\n",
    "        new_edges_in_extended = []\n",
    "        for u, v, data in G_extended.edges(data=True):\n",
    "            if data.get('origin') == 'predicha': # Asumiendo que 'origin' es una propiedad que marca las aristas predichas\n",
    "                new_edges_in_extended.append((u, v, data.get('relation', 'unknown')))\n",
    "\n",
    "        if new_edges_in_extended:\n",
    "            strong_new_edges_count = 0\n",
    "            if best_ontology_report: # Use the validator from the best ontology for detailed check\n",
    "                detailed_validator = GraphValidator(ontology_urls=[ontologies_map[best_ontology_name]])\n",
    "                for u, v, rel in new_edges_in_extended:\n",
    "                    edge_validation_result = detailed_validator.validate_edge(u, v, rel)\n",
    "                    if edge_validation_result['fully_valid']:\n",
    "                        strong_new_edges_count += 1\n",
    "\n",
    "            print(f\"\\nüîç An√°lisis de {len(new_edges_in_extended)} aristas predichas a√±adidas:\")\n",
    "            print(f\"   ‚Ä¢ Validadas ontol√≥gicamente (por {best_ontology_name}): {strong_new_edges_count} ({(strong_new_edges_count/len(new_edges_in_extended))*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\nüîç No se encontraron aristas predichas a√±adidas al grafo extendido para analizar.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V5E1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
